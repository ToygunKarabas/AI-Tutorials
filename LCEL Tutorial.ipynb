{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOllama(model=\"llama3.1:latest\",\n",
    "#                  temperature=0.3,\n",
    "#                  format=\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3-groq-tool-use:8b\",\n",
    "                 temperature=0.3,\n",
    "                 format=\"json\")\n",
    "\n",
    "# llm = ChatGroq(model=\"llama3-groq-70b-8192-tool-use-preview\", # \"llama3-70b-8192\"\n",
    "#                temperature=0.3,\n",
    "#                api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"Give me small report about {topic}\"\"\"\n",
    "\n",
    "prompt_template=PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "response = llm_chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's a brief report on Artificial Intelligence:\\n\\n**Overview**\\n\\nArtificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and problem-solving. AI has been around for decades, but recent advances in machine learning algorithms and data storage have made it increasingly powerful.\\n\\n**Key Features**\\n\\n* **Machine Learning**: AI systems use complex algorithms to learn from data and improve their performance over time.\\n* **Natural Language Processing**: AI can understand and generate human language, enabling applications like chatbots and voice assistants.\\n* **Computer Vision**: AI can interpret and understand visual information from images and videos.\\n* **Robotics**: AI is used in robotics to control and navigate robots that interact with physical environments.\\n\\n**Applications**\\n\\n* **Virtual Assistants**: AI-powered virtual assistants like Siri, Google Assistant, and Alexa have revolutionized the way we interact with technology.\\n* **Image Recognition**: AI-based image recognition has numerous applications in security, healthcare, and entertainment.\\n* **Self-Driving Cars**: Autonomous vehicles rely on AI to navigate roads and avoid obstacles.\\n* **Predictive Maintenance**: AI-powered predictive maintenance helps industries like manufacturing and healthcare anticipate equipment failures.\\n\\n**Challenges**\\n\\n* **Bias and Fairness**: AI systems can perpetuate biases present in the data used to train them, leading to unfair outcomes.\\n* **Explainability**: As AI becomes more complex, it's increasingly difficult to understand how decisions are made.\\n* **Job Displacement**: The automation of routine tasks has raised concerns about job displacement and economic disruption.\\n\\n**Future Outlook**\\n\\nAI is expected to continue transforming industries and improving lives. While there are challenges to be addressed, the benefits of AI in areas like healthcare, education, and transportation promise to be significant. As AI becomes more integrated into daily life, we can expect to see even more innovative applications emerge.\", response_metadata={'model': 'llama3.1:latest', 'created_at': '2024-07-25T17:46:02.016745Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 6836633875, 'load_duration': 28566333, 'prompt_eval_count': 17, 'prompt_eval_duration': 219291000, 'eval_count': 386, 'eval_duration': 6587842000}, id='run-fb85b410-eb0c-46be-9635-1ea22d70734d-0', usage_metadata={'input_tokens': 17, 'output_tokens': 386, 'total_tokens': 403})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give me small report about AI'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Give me small report about {topic}\".format(topic=\"Artificial Intelligence\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "==============\n",
      "\n",
      "{'model': 'llama3.1:latest', 'created_at': '2024-07-25T17:46:02.016745Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 6836633875, 'load_duration': 28566333, 'prompt_eval_count': 17, 'prompt_eval_duration': 219291000, 'eval_count': 386, 'eval_duration': 6587842000}\n",
      "\n",
      "==============\n",
      "\n",
      "{'input_tokens': 17, 'output_tokens': 386, 'total_tokens': 403}\n",
      "\n",
      "==============\n",
      "\n",
      "Here's a brief report on Artificial Intelligence:\n",
      "\n",
      "**Overview**\n",
      "\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and problem-solving. AI has been around for decades, but recent advances in machine learning algorithms and data storage have made it increasingly powerful.\n",
      "\n",
      "**Key Features**\n",
      "\n",
      "* **Machine Learning**: AI systems use complex algorithms to learn from data and improve their performance over time.\n",
      "* **Natural Language Processing**: AI can understand and generate human language, enabling applications like chatbots and voice assistants.\n",
      "* **Computer Vision**: AI can interpret and understand visual information from images and videos.\n",
      "* **Robotics**: AI is used in robotics to control and navigate robots that interact with physical environments.\n",
      "\n",
      "**Applications**\n",
      "\n",
      "* **Virtual Assistants**: AI-powered virtual assistants like Siri, Google Assistant, and Alexa have revolutionized the way we interact with technology.\n",
      "* **Image Recognition**: AI-based image recognition has numerous applications in security, healthcare, and entertainment.\n",
      "* **Self-Driving Cars**: Autonomous vehicles rely on AI to navigate roads and avoid obstacles.\n",
      "* **Predictive Maintenance**: AI-powered predictive maintenance helps industries like manufacturing and healthcare anticipate equipment failures.\n",
      "\n",
      "**Challenges**\n",
      "\n",
      "* **Bias and Fairness**: AI systems can perpetuate biases present in the data used to train them, leading to unfair outcomes.\n",
      "* **Explainability**: As AI becomes more complex, it's increasingly difficult to understand how decisions are made.\n",
      "* **Job Displacement**: The automation of routine tasks has raised concerns about job displacement and economic disruption.\n",
      "\n",
      "**Future Outlook**\n",
      "\n",
      "AI is expected to continue transforming industries and improving lives. While there are challenges to be addressed, the benefits of AI in areas like healthcare, education, and transportation promise to be significant. As AI becomes more integrated into daily life, we can expect to see even more innovative applications emerge.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(\"\\n==============\\n\")\n",
    "print(response.response_metadata)\n",
    "print(\"\\n==============\\n\")\n",
    "print(response.usage_metadata)\n",
    "print(\"\\n==============\\n\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Answer the user prompt below using the chat history provided below.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{user_prompt}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "previous_ai_message = response.content\n",
    "\n",
    "messages = [HumanMessage(\"Give me small report about Artificial Intelligence\"),\n",
    "            AIMessage(previous_ai_message)]\n",
    "\n",
    "system_template = \"\"\"Answer the user prompt below using the chat history provided below.\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{user_prompt}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "\n",
      "==============\n",
      "\n",
      "[SystemMessage(content='Answer the user prompt below using the chat history provided below.'), HumanMessage(content='Give me small report about Artificial Intelligence'), AIMessage(content=\"Here's a brief report on Artificial Intelligence:\\n\\n**Overview**\\n\\nArtificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and problem-solving. AI has been around for decades, but recent advances in machine learning algorithms and data storage have made it increasingly powerful.\\n\\n**Key Features**\\n\\n* **Machine Learning**: AI systems use complex algorithms to learn from data and improve their performance over time.\\n* **Natural Language Processing**: AI can understand and generate human language, enabling applications like chatbots and voice assistants.\\n* **Computer Vision**: AI can interpret and understand visual information from images and videos.\\n* **Robotics**: AI is used in robotics to control and navigate robots that interact with physical environments.\\n\\n**Applications**\\n\\n* **Virtual Assistants**: AI-powered virtual assistants like Siri, Google Assistant, and Alexa have revolutionized the way we interact with technology.\\n* **Image Recognition**: AI-based image recognition has numerous applications in security, healthcare, and entertainment.\\n* **Self-Driving Cars**: Autonomous vehicles rely on AI to navigate roads and avoid obstacles.\\n* **Predictive Maintenance**: AI-powered predictive maintenance helps industries like manufacturing and healthcare anticipate equipment failures.\\n\\n**Challenges**\\n\\n* **Bias and Fairness**: AI systems can perpetuate biases present in the data used to train them, leading to unfair outcomes.\\n* **Explainability**: As AI becomes more complex, it's increasingly difficult to understand how decisions are made.\\n* **Job Displacement**: The automation of routine tasks has raised concerns about job displacement and economic disruption.\\n\\n**Future Outlook**\\n\\nAI is expected to continue transforming industries and improving lives. While there are challenges to be addressed, the benefits of AI in areas like healthcare, education, and transportation promise to be significant. As AI becomes more integrated into daily life, we can expect to see even more innovative applications emerge.\"), HumanMessage(content='what was your first sentence in your previous response.')]\n"
     ]
    }
   ],
   "source": [
    "prompt_value = prompt_template.invoke({\"user_prompt\": \"what was your first sentence in your previous response.\",\n",
    "                                       \"chat_history\": messages})\n",
    "\n",
    "print(type(prompt_value))\n",
    "print(\"\\n==============\\n\")\n",
    "print(prompt_value.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first sentence in my previous response was:\n",
      "\n",
      "\"Here's a brief report on Artificial Intelligence:\"\n"
     ]
    }
   ],
   "source": [
    "llm_chain = prompt_template | llm\n",
    "\n",
    "response = llm_chain.invoke({\"user_prompt\": \"what was your first sentence in your previous response.\",\n",
    "                            \"chat_history\": messages})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda & RunnableParallel & RunnablePassThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++START++\n",
      "Here's a brief report on Artificial Intelligence:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI has been rapidly evolving over the past few decades and has become an essential part of our daily lives.\n",
      "\n",
      "**History**\n",
      "\n",
      "The concept of AI dates back to ancient civilizations, but the modern era of AI began in the 1950s with the development of the first computer programs that could simulate human thought processes. The Dartmouth Summer Research Project on Artificial Intelligence in 1956 is considered a significant milestone in the history of AI.\n",
      "\n",
      "**Key Aspects**\n",
      "\n",
      "There are several key aspects of AI:\n",
      "\n",
      "1. **Machine Learning**: This involves training algorithms to learn from data and improve their performance over time.\n",
      "2. **Natural Language Processing (NLP)**: This enables computers to understand, interpret, and generate human language.\n",
      "3. **Computer Vision**: This allows computers to interpret and understand visual information from images and videos.\n",
      "\n",
      "**Applications**\n",
      "\n",
      "AI has a wide range of applications across various industries, including:\n",
      "\n",
      "1. **Healthcare**: AI can help diagnose diseases, develop personalized treatment plans, and streamline medical research.\n",
      "2. **Finance**: AI can analyze vast amounts of financial data to predict market trends, detect fraud, and optimize investment portfolios.\n",
      "3. **Transportation**: AI can improve traffic flow, predict traffic patterns, and enable self-driving cars.\n",
      "\n",
      "**Benefits**\n",
      "\n",
      "The benefits of AI include:\n",
      "\n",
      "1. **Increased Efficiency**: AI can automate repetitive tasks, freeing up human resources for more complex and creative work.\n",
      "2. **Improved Accuracy**: AI can analyze vast amounts of data to make accurate predictions and decisions.\n",
      "3. **Enhanced Customer Experience**: AI can provide personalized customer service, improve product recommendations, and optimize supply chains.\n",
      "\n",
      "**Challenges**\n",
      "\n",
      "Despite its many benefits, AI also raises several challenges, including:\n",
      "\n",
      "1. **Job Displacement**: AI may displace certain jobs, particularly those that involve repetitive or routine tasks.\n",
      "2. **Bias and Fairness**: AI systems can perpetuate existing biases and inequalities if they are trained on biased data.\n",
      "3. **Security Risks**: AI systems can be vulnerable to cyber attacks and other security threats.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Artificial Intelligence has the potential to revolutionize many industries and improve our daily lives in numerous ways. However, it also raises important challenges that must be addressed through careful consideration of its benefits and drawbacks.\n",
      "++END++\n"
     ]
    }
   ],
   "source": [
    "def change_response(res):\n",
    "    return \"++START++\\n\" + res.content + \"\\n++END++\"\n",
    "\n",
    "change_response_func = RunnableLambda(change_response)\n",
    "\n",
    "\n",
    "template=\"\"\"Give me small report about {topic}\"\"\"\n",
    "\n",
    "prompt_template=PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = prompt_template | llm | change_response_func\n",
    "\n",
    "response = llm_chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sentence using all three words:\n",
      "\n",
      "The lion, famished from its long hunt, devoured its prey with great relish, savoring the taste of fresh meat and the rich, crimson blood that still pulsed within its lifeless body.\n"
     ]
    }
   ],
   "source": [
    "def get_animal(a):\n",
    "    return \"lion\"\n",
    "\n",
    "def get_random_word(b):\n",
    "    return \"meat\"\n",
    "\n",
    "change_response_func = RunnableLambda(change_response)\n",
    "\n",
    "\n",
    "template=\"\"\"Use given words in one sentence: {animal} and {random_word}. Also consider the word provided by user: {user_prompt}\"\"\"\n",
    "\n",
    "prompt_template=PromptTemplate.from_template(template)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"animal\": RunnableLambda(get_animal), \"random_word\": RunnableLambda(get_random_word),\n",
    "        \"user_prompt\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "llm_chain = retrieval | prompt_template | llm\n",
    "\n",
    "response = llm_chain.invoke(\"blood\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 0}, page_content='Categorical-and-numerical-attribute data clustering based on a uniﬁed\\nsimilarity metric without knowing cluster number\\nYiu-ming Cheunga,b,n, Hong Jiaa\\naDepartment of Computer Science and Institute of Computational and Theoretical Studies, Hong Kong Baptist University, Hong Kong, China\\nbUnited International College, Beijing Normal University-Hong Kong Baptist University, Zhuhai, China\\narticle info\\nArticle history:\\nReceived 19 March 2012Received in revised form29 December 2012Accepted 23 January 2013\\nAvailable online 31 January 2013\\nKeywords:\\nClustering\\nSimilarity metricCategorical attributeNumerical attributeNumber of clustersabstract\\nMost of the existing clustering approaches are applicable to purely numerical or categorical data only,\\nbut not the both. In general, it is a nontrivial task to perform clustering on mixed data composed of\\nnumerical and categorical attributes because there exists an awkward gap between the similaritymetrics for categorical and numerical data. This paper therefore presents a general clustering frame-\\nwork based on the concept of object-cluster similarity and gives a uniﬁed similarity metric which can\\nbe simply applied to the data with categorical, numerical, and mixed attributes. Accordingly, aniterative clustering algorithm is developed, whose outstanding performance is experimentally demon-\\nstrated on different benchmark data sets. Moreover, to circumvent the difﬁcult selection problem of\\ncluster number, we further develop a penalized competitive learning algorithm within the proposedclustering framework. The embedded competition and penalization mechanisms enable this improvedalgorithm to determine the number of clusters automatically by gradually eliminating the redundant\\nclusters. The experimental results show the efﬁcacy of the proposed approach.\\n&2013 Elsevier Ltd. All rights reserved.\\n1. Introduction\\nTo discover the natural group structure of objects represented\\nin numerical or categorical attributes [1], clustering analysis has\\nbeen widely applied to a variety of scientiﬁc areas such as\\ncomputer science [2]and bioinformatics [3]. Traditionally, clus-\\ntering analysis concentrates on purely numerical data only. The\\ntypical clustering algorithms include the k-means [4], EM algo-\\nrithm [5]and their variants. Since the objective functions of these\\ntwo algorithms are both numerically deﬁned, they are not\\nessentially applicable to the data sets with categorical attributes.\\nUnder the circumstances, a straightforward way to overcome this\\nproblem is to transform the categorical values into numerical\\nones, e.g. the binary strings, and then apply the aforementioned\\nnumerical-value based clustering methods. Nevertheless, such a\\nmethod has ignored the similarity information embedded in the\\ncategorical values and cannot faithfully reveal the similarity\\nstructure of the data sets [6]. Hence, it is desirable to solve this\\nproblem by ﬁnding a uniﬁed similarity metric for categorical and\\nnumerical attributes such that the metric gap between numerical\\nand categorical data can be eliminated. Subsequently, a general\\nclustering algorithm which is applicable to numerical andcategorical data can be presented based on this uniﬁed metric.\\nDuring the past decades, some works which try to ﬁnd a uniﬁed\\nsimilarity metric for categorical and numerical attributes havebeen presented, e.g. see [7]. However, a computational efﬁcient\\nsimilarity measure remains to be developed.\\nAnother challenging problem encountered in clustering is how\\nto determine the number of clusters. To the best of our knowl-\\nedge, a lot of popular clustering methods, e.g. the k-means\\nalgorithm for numerical data clustering and the k-modes algo-\\nrithm [8]for categorical data clustering, need to pre-assign the\\nnumber of clusters exactly. Otherwise, they will almost always\\nlead to a poor clustering result [9,10]. Unfortunately, in many\\ncases, this vital information is not always available from the\\npractical viewpoint. Hence, to explore an algorithm which can\\nconduct clustering without knowing cluster number is also a\\nsigniﬁcant work in clustering analysis. To address this issue,\\nvariant researches have been conducted in the literature and\\nsome feasible methods that can determine the number of clusters\\nfor purely numerical or categorical data have been presented\\n[9–11]. Nevertheless, to the best of our knowledge, how to\\nautomatically select cluster number for mixed data during clus-\\ntering process is still an unsolved problem.\\nIn this paper, we will propose a uniﬁed clustering approach\\nthat is capable of selecting the cluster number automatically for\\nboth categorical and numeric data sets. Firstly, we present a\\ngeneral clustering framework based on the concept of object-Contents lists available at SciVerse ScienceDirect\\njournal homepage: www.elsevier.com/locate/prPattern Recognition\\n0031-3203/$ - see front matter &2013 Elsevier Ltd. All rights reserved.\\nhttp://dx.doi.org/10.1016/j.patcog.2013.01.027nCorresponding author. Tel.: þ852 34115155.\\nE-mail addresses: ymc@comp.hkbu.edu.hk (Y.-m. Cheung),\\nhjia@comp.hkbu.edu.hk (H. Jia).Pattern Recognition 46 (2013) 2228–2238'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 1}, page_content='cluster similarity. Then, a new metric for both of numerical and\\ncategorical attributes is proposed. Under this metric, the object-\\ncluster similarity for either categorical or numerical attributes has\\na uniform criterion. Hence, transformation and parameter adjust-\\nment between categorical and numerical values in data clustering\\nare circumvented. Subsequently, an iterative clustering algorithm\\nis introduced. This algorithm conducts a parameter-free cluster-\\ning analysis and is applicable to the three types of data: numer-\\nical, categorical, or mixed data, i.e., the data with the both of\\nnumerical and categorical attributes. Moreover, empirical studies\\nshow that the proposed algorithm has higher accuracy as well as\\nlower computational cost compared to the popular k-modes\\nalgorithm for categorical data clustering. For mixed data cluster-\\ning, compared to k-prototype algorithm [12], the proposed\\nmethod can get much better clustering results, but no parameter\\nneeds to be adjusted at all. Additionally, to overcome the cluster\\nnumber selection problem, we further present a penalized com-\\npetitive learning algorithm within the proposed clustering frame-\\nwork. The competition and penalization mechanisms in this\\nimproved algorithm can gradually fade out the redundant clus-\\nters. Hence, the number of clusters can be determined automa-\\ntically during the clustering process. Experimental results on\\nbenchmark data sets have shown the effectiveness of this method.\\nThe rest of this paper is organized as follows. Related works\\nare reviewed in Section 2 .Section 3 proposes a general clustering\\nframework based on object-cluster similarity, whose metric is\\nalso deﬁned. Section 4 describes an iterative clustering algorithm\\nand Section 5 presents an improved one with capability of\\nautomatically selecting cluster number. Experiments are con-\\nducted in Section 6 . Finally, we draw a conclusion in Section 7 .\\n2. Related works\\nThis section reviews the related works on: (1) data clustering\\nwith categorical-and-numerical attributes and (2) cluster number\\nselection.\\nIn the former, several methods have been presented which can\\nbe grouped into two lines. In the ﬁrst line, the algorithms are\\nessentially designed for purely categorical data, although they\\nhave been applied to the mixed data as well by transforming the\\nnumerical attributes to categorical ones via a discretization\\nmethod. Along this line, several methods have been proposed\\nbased on the perspective of similarity metric, graph partitioning\\nor information entropy. For example, ROCK algorithm proposed\\nby Guha et al. [13] is an agglomerative hierarchical clustering\\nprocedure based on the concepts of neighbors and links. In this\\nmethod, a pair of objects are regarded as neighbors if their\\nsimilarity exceeds a certain threshold, and the desired cluster\\nstructure is obtained by merging the clusters sharing a pre-\\nassigned number of neighbors gradually. ROCK has shown its\\nsuperiority over traditional hierarchical algorithms in the experi-\\nments, but its performance is generally sensitive to the setting of\\nsimilarity threshold. Also, the computation of links between\\nobjects is quite time-consuming [14]. By contrast, CLICKS algo-\\nrithm proposed in [15] mines subspace clusters for categorical\\ndata sets. This method encodes a data set into a weighted graph\\nstructure, where each weighted vertex stands for an attribute\\nvalue and two nodes are connected if there is a sample in which\\nthe corresponding attribute values co-occur. Experiments haveshown that CLICKS outperforms ROCK algorithm and scales better\\nfor high-dimensional data sets. However, its performance also\\ndepends upon a set of parameters whose tuning is quite difﬁcult\\nfrom the practical viewpoint. Additionally, the COOLCAT algo-\\nrithm, an entropy-based method proposed by Barbara et al. [16],\\nutilizes the information entropy to measure the closenessbetween objects and presents a scheme to ﬁnd a clustering\\nstructure via minimizing the expected entropy of clusters. The\\nperformance of this algorithm is stable for different data sizes and\\nparameter settings. Furthermore, a scalable algorithm for catego-\\nrical data clustering called LIMBO [17], which is proposed based\\non the Information Bottleneck (IB) framework [18], employs the\\nconcept of mutual information to ﬁnd a clustering with minimum\\ninformation loss. In general, all of the above-stated algorithms can\\nbe applied to mixed data via a discretization process, which may,\\nhowever, cause loss of important information, e.g. the differencebetween numerical values.\\nBy contrast, the second line attempts to design a generalized\\nclustering criterion for numerical-and-categorical attributes. For\\nexample, Li and Biswas [7] presented the Similarity Based\\nAgglomerative Clustering (SBAC) algorithm which is based on\\nGoodall similarity metric [19] that assigns a greater weight to\\nuncommon feature value matching in similarity computations\\nwithout the prior knowledge of the underlying distributions of\\nthe feature values. This method has a good capability of dealing\\nwith the mixed attributes, but its computation is quite laborious.\\nHe et al. [20] extended the Squeezer algorithm to cluster mixed\\ndata and proposed the usm-squeezer method, in which the\\nsimilarity measure for categorical attributes is the same as the\\nSqueezer while the similarity of numerical attributes is deﬁned by\\nrelative difference. However, the clustering effectiveness of this\\nmethod has not been sufﬁciently demonstrated. In [21],\\nan Evidence-Based Spectral Clustering (EBSC) algorithm has been\\nproposed for mixed data clustering by integrating the evidence\\nbased similarity metric into the spectral clustering structure.\\nMoreover, the AUTOCLASS proposed by Cheeseman and Stutz\\n[22] assumes a classical ﬁnite mixture distribution model on\\nmixed data and utilizes a Bayesian method to derive the most\\nprobable class distribution for the data given prior information.Among this category of approaches, the most cost-effective one\\nmay be the k-prototype algorithm proposed by Huang [12]. In this\\nmethod, the distance between two categorical values is deﬁned as\\n0 if they are the same, and 1 otherwise while the distance\\nbetween numerical values are quantiﬁed with Euclidean distance.\\nSubsequently, the k-means paradigm is utilized for clustering.\\nHowever, since different metrics are adopted for numerical and\\ncategorical attributes, a user-deﬁned parameter is utilized to\\ncontrol the proportions of numerical distance and categorical\\ndistance. Nevertheless, the clustering result is very sensitive to\\nthe setting of this parameter. A simpliﬁed version of k-prototype\\nalgorithm namely k-modes [8,23,24], which is applicable for\\npurely categorical data clustering, has also been widely utilized.\\nThus far, different improvement strategies on this method have\\nbeen explored, e.g. see [25–27].\\nIn general, all of the aforementioned methods need to pre-\\nassign the number of clusters exactly, which is, however,\\na nontrivial task from the practical viewpoint. In the literature,\\na variety of methods have been proposed for cluster number\\nestimation. For example, some computational demanding meth-\\nods choose the optimal number of clusters via different statistic\\ncriteria, such as Akaike’s Information Criterion (AIC) [28] and\\nSchwarz’s Bayesian inference criterion (BIC) [29]. By contrast,\\nanother kind of methods within the framework of competitive\\nlearning often introduce some competitive mechanisms, such as\\npenalization [9,11] and cooperation [30], into the clustering\\nprocess so that the number of clusters can be automatically\\nselected. Nevertheless, these existing methods focus on numerical\\ndata only and cannot be directly applied to data sets with\\ncategorical attributes. Recently, Liao and Ng [10] have introduced\\nan entropy penalty term into the objective function of k-modes\\nalgorithm. Then, by choosing different values for the regulariza-\\ntion parameter, variant clustering results with different clusterY.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2229'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 2}, page_content='numbers can be obtained. Subsequently, the cluster number\\naccompanying with the most stable clustering result is selected.\\nAs the learning process needs to be repeated for a large range of\\nvalues of regulation parameter, the computation of this method is\\nmuch more expensive than the original k-modes algorithm.\\n3. Clustering problem and object-cluster similarity metric\\nThe general task of clustering is to classify the given objects\\ninto several clusters such that the similarities between objects in\\nthe same group are high while the similarities between objects in\\ndifferent groups are low [31,32]. Therefore, clustering a set of N\\nobjects (also called inputs interchangeably), fx1,x2,... ,xNg, into k\\ndifferent clusters, denoted as C1,C2,... ,Ck, can be formulated to\\nﬁnd the optimal Qnvia the following objective function:\\nQn¼arg max\\nQFðQÞ¼arg max\\nQXk\\nj¼1XN\\ni¼1qijsðxi,CjÞ2\\n43\\n5, ð1Þ\\nwhere sðxi,CjÞis the similarity between object xiand Cluster Cj,\\nandQ¼ðqijÞis an N/C2kpartition matrix satisfying\\nXk\\nj¼1qij¼1,and 0oXN\\ni¼1qijoN, ð2Þ\\nwith\\nqijA½0,1/C138,i¼1,2,... ,N,j¼1,2,... ,k: ð3Þ\\nEvidently, the desired clusters can be obtained by Eq. (1)as long as\\nthe metric of object-cluster similarity is determined. In the following\\nsub-sections, we shall therefore study the similarity metric.\\n3.1. Similarity metric for mixed data\\nThis sub-section will study the object-cluster similarity metric\\nfor mixed data. Suppose the mixed data xiwith ddifferent\\nattributes consists of dccategorical attributes and dunumerical\\nattributes, i.e., dcþduþd. Hence, x_i can be denoted as ½xc\\niT,xu\\niT/C138T\\nwith xc\\ni¼ðxc\\ni1,xc\\ni2,... ,xc\\nidcÞTand xu\\ni¼ðxu\\ni1,xu\\ni2,... ,xu\\niduÞT. Then, xu\\nir\\nðr¼1,2,... ,duÞbelongs to Rand xc\\nirðr¼1,2,... ,dcÞbelongs to\\ndom ðArÞ, where fA1,A2,... ,Adcgare the dccategorical attributes and\\ndom ðArÞcontains all the possible values that can be chosen by\\nattribute Ar. For categorical attributes, as the value domains are\\nﬁnite and unordered, dom ðArÞwith mrelements can be repre-\\nsented with dom ðArÞ¼far1,ar2,... ,armrg.\\nFirstly, we focus on the difference between categorical attri-\\nbutes and numerical attributes. For categorical attributes, each\\nattribute can usually represent an important feature of the given\\nobject. Therefore, when we conduct classiﬁcation or clustering\\nanalysis, we often investigate the categorical attributes one by\\none such as Decision Tree method. By contrast, the numerical\\nattributes are often treated as a vector and handled together in\\nclustering analysis. That is, we pay more attention to the total\\neffect of numerical attributes. Based on these observations, for the\\nmixed data xi, the numerical part xu\\nican be treated as a whole but\\nthe dccategorical attributes should be investigated individually.\\nConsequently, although the dimensionality of xiisd, the number\\nof features that contributes to clustering analysis will be dcþ1\\n(i.e., dccategorical features and 1 numerical vector). Let the\\nobject-cluster similarity between xiand cluster Cj, denoted as\\nsðxi,CjÞ, be the average of the similarity calculated based on each\\nfeature, we will then have\\nsðxi,CjÞ¼1\\ndf½sðxc\\ni1,CjÞþsðxc\\ni2,CjÞþ /C1/C1/C1 þ sðxc\\nidc,CjÞþsðxu\\ni,CjÞ/C138¼1\\ndfXdc\\nr¼1sðxc\\nir,CjÞþ1\\ndfsðxu\\ni,CjÞ, ð4Þ\\nwhere df¼dcþ1. If we denote the similarity between xc\\niand Cjas\\nsðxc\\ni,CjÞ, we can get\\nsðxc\\ni,CjÞ¼1\\ndcXdc\\nr¼1sðxc\\nir,CjÞ¼Xdc\\nr¼11\\ndcsðxc\\nir,CjÞ: ð5Þ\\nThen, Eq. (4)can be further rewritten as\\nsðxi,CjÞ¼dc\\ndfXdc\\nr¼11\\ndcsðxc\\nir,CjÞþ1\\ndfsðxu\\ni,CjÞ¼dc\\ndfsðxc\\ni,CjÞþ1\\ndfsðxu\\ni,CjÞ,\\nð6Þ\\nwhere sðxc\\ni,CjÞis the similarity on categorical attributes and\\nsðxu\\ni,CjÞis the similarity on numerical attributes. Subsequently,\\nthe object-cluster similarity metric can be obtained based on the\\ndeﬁnitions of sðxc\\ni,CjÞand sðxu\\ni,CjÞ.\\n3.1.1. Similarity metric for categorical attributes\\nIn Eq. (5)we have assumed that each categorical attribute has\\nthe same contribution to the calculation of similarity on catego-\\nrical part. But in practice, due to the different distributions of\\nattribute values, categorical attributes each often have unequal\\nimportance for clustering analysis. In light of this characteristic,\\nEq.(5)should be further modiﬁed with\\nsðxc\\ni,CjÞ¼Xdc\\nr¼1wrsðxc\\nir,CjÞ, ð7Þ\\nwhere wris the weight of categorical attribute Arsatisfying\\n0rwrr1 andPdc\\nr¼1wr¼1. That is, the object-cluster similarity\\nfor categorical part is the weighted summation of the similarity\\nbetween the cluster and each attribute value. The weight factor wr\\ndescribes the importance of each categorical attribute and is\\nutilized to control the contribution of attribute-cluster similarity\\nto object-cluster similarity.\\nDeﬁnition 1. The similarity between a categorical attribute value\\nxc\\nirand Cluster Cj,iAf1,2,... ,Ng,rAf1,2,... ,dcg,jAf1,2,... ,kg,i s\\ndeﬁned as\\nsðxc\\nir,CjÞ¼sAr¼xc\\nirðCjÞ\\nsAraNULLðCjÞ, ð8Þ\\nwhere sAr¼xc\\nirðCjÞcounts the number of objects (also called\\ninstances hereinafter) in Cluster Cjthat have the value xc\\nirfor\\nattribute Ar, NULL refers to the empty, and sAraNULLðCjÞmeans the\\nnumber of objects in Cluster Cjthat have the attribute Arwhose\\nvalue is not equal to NULL.\\nFrom Deﬁnition 1 , we can ﬁnd that this metric of attribute-\\ncluster similarity is deﬁned from probabilistic viewpoint and has\\nthe following properties:\\n(1) 0rsðxc\\nir,CjÞr1;\\n(2)sðxc\\nir,CjÞ¼1 only if all the instances belonging to Cluster Cjhave\\nthe value xc\\nirfor attribute Ar,a n d sðxc\\nir,CjÞ¼0 only if no instance\\nbelonging to Cluster Cjhas the value xc\\nirfor attribute Ar.\\nAccording to Eqs. (7) and (8) , the object-cluster similarity for\\ncategorical part can be therefore calculated by\\nsðxc\\ni,CjÞ¼Xdc\\nr¼1wrsðxc\\nir,CjÞ¼Xdc\\nr¼1wrsAr¼xc\\nirðCjÞ\\nsAraNULLðCjÞ, ð9Þ\\nwhere iAf1,2,... ,Ngand jAf1,2,... ,kg.Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2230'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 3}, page_content='Remark 1. Since 0rsðxc\\nir,CjÞr1 andPdc\\nr¼1wr¼1, we have\\nsðxc\\ni,CjÞ¼Xdc\\nr¼1wrsðxc\\nir,CjÞZXdc\\nr¼1ðwr/C10Þ¼0,\\nand\\nsðxc\\ni,CjÞ¼Xdc\\nr¼1wrsðxc\\nir,CjÞrXdc\\nr¼1ðwr/C11Þ¼Xdc\\nr¼1wr¼1:\\nThat is, for any iAf1,2,... ,Ngand jAf1,2,... ,kg, the value of\\nsðxc\\ni,CjÞwill fall into the interval ½0,1/C138.\\nNext, we discuss how to estimate the importance of each\\ncategorical attribute. From the view point of information theory,\\nthe signiﬁcance of an attribute can be regarded as the inhomo-\\ngeneity degree of the data set with respect to this attribute.\\nFurthermore, according to the Measure III proposed in [33], if the\\ninformation content of an attribute is high, the inhomogeneity of\\nthe data set is also high for this attribute. Hence, the importance\\nof an arbitrary attribute Acan be quantiﬁed by the following\\nentropy metric:\\nHA¼/C0Z\\npðxðAÞÞlogðpðxðAÞÞÞdxðAÞ, ð10Þ\\nwhere x(A) is the value of attribute A, and pðxðAÞÞis the probability\\ndensity function of x(A) along this dimension. For categorical\\nattributes, since the possible attribute values are ﬁnite, discrete\\nand independent, the information content of an attribute can be\\nestimated by the average information content of all possible\\nattribute values and the probability of each attribute value can\\nbe computed by counting its frequency in the whole data set.\\nConsequently, the importance of any categorical attribute Ar\\nðrAf1,2,... ,dcgÞcan be calculated by\\nHAr¼/C0Xmr\\nt¼1pðartÞlogpðartÞ, ð11Þ\\nwith\\npðartÞ¼sAr¼artðXÞ\\nsAraNULLðXÞ, ð12Þ\\nwhere artAdom ðArÞ,mris the total number of values that can be\\nchosen by Arand Xis the whole data set. Furthermore, according\\nto Eq. (11), the more different values an attribute has, the higher\\nits signiﬁcance is. However, in practice, an attribute with too\\nmany different values may have little contribution to clustering.\\nFor example, the ID number of instances is unique for each\\ninstance, but this information is useless for clustering analysis.\\nHence, Eq. (11) can be further modiﬁed with\\nHAr¼/C01\\nmrXmr\\nt¼1pðartÞlogpðartÞ: ð13Þ\\nThat is, the importance of an attribute is quantiﬁed by its average\\nentropy over each attribute value. The weight of each attribute is\\nthen computed as\\nwr¼HArPdc\\nt¼1HAt,r¼1,2,... ,dc: ð14Þ\\nSubsequently, the object-cluster similarity on categorical part can\\nbe given by\\nsðxc\\ni,CjÞ¼Xdc\\nr¼1HArPdc\\nt¼1HAt/C1sAr¼xc\\nirðCjÞ\\nsAraNULLðCjÞ !\\n: ð15Þ\\nIn practice, for an attribute Ar, if all the instances to be classiﬁed\\nhave the same value a, it can be obtained from Eqs. (12) and (13)\\nthat the importance of this attribute will be zero as pðaÞ¼1. Then,\\nthe corresponding attribute weight will also be zero. This impliesthat this attribute will have no contribution at all to the whole\\nclustering learning.\\n3.1.2. Similarity metric for numerical attributes\\nSince the distance between each vector xu\\nican be numerically\\ncalculated, the similarity metric for numerical attributes can be\\ndeﬁned based on the measure of distance.\\nDeﬁnition 2. The object-cluster similarity between numerical\\nvector xu\\niand cluster Cj,iAf1,2,... ,Ng,jAf1,2,... ,kg, is given by\\nsðxu\\ni,CjÞ¼expð/C00:5Disðxu\\ni,cjÞÞ\\nPk\\nt¼1expð/C00:5Disðxu\\ni,ctÞÞ, ð16Þ\\nwhere cjis the center of all numerical vectors in cluster Cjand\\nDisð/C1Þstands for a distance function. It can be seen that the values\\nof this similarity metric also fall into the interval [0, 1].\\nIn practice, if the Mahalanobis distance metric is adopted, we\\nwill have\\nDisðxu\\ni,cjÞ¼ðxu\\ni/C0cjÞTS/C01\\njðxu\\ni/C0cjÞ, ð17Þ\\nwhere Sjis the covariance matrix of numerical vectors in jth\\ncluster. Further, if we utilize the Euclidean distance, the similarity\\nmetric can become\\nsðxu\\ni,CjÞ¼expð/C00:5Jxu\\ni/C0cjJ2Þ\\nPk\\nt¼1expð/C00:5Jxu\\ni/C0ctJ2Þ: ð18Þ\\nActually, it can be derived that this similarity metric is equivalent\\nto the posterior probability of xu\\nibelonging to cluster Cjprovided\\nthat the probability density function of each vector is a mixture of\\nstandard normal distribution with equal mixture coefﬁcients.\\n3.2. Object-cluster similarity metric\\nAccording to Eqs. (6), (15) and (16) , the object-cluster similar-\\nity metric for mixed data is deﬁned as\\nsðxi,CjÞ¼dc\\ndfsðxc\\ni,CjÞþ1\\ndfsðxu\\ni,CjÞ¼dc\\ndfXdc\\nr¼1HArPdc\\nt¼1HAt/C1sAr¼xc\\nirðCjÞ\\nsAraNULLðCjÞ !\\nþ1\\ndfexpð/C00:5Disðxu\\ni,cjÞÞ\\nPk\\nt¼1expð/C00:5Disðxu\\ni,ctÞÞ, ð19Þ\\nwhere i¼1,2,... ,N,j¼1,2,... ,k. It can be seen that the deﬁned\\nsimilarities for categorical and numerical attributes in Eq. (19) are\\nin the same scale. That is, the values for sðxc\\ni,CjÞand sðxu\\ni,CjÞare\\nwithin the interval ½0,1/C138. Hence, unlike k-prototype method,\\nadditional parameters to control the proportions of numerical\\nand categorical distances are not needed any more.\\nSpecially, if the data to be classiﬁed contain categorical\\nattributes only, there does not exist the numerical vector xu\\niof\\neach object xi. Then, we can get xi¼xc\\ni,dc¼dand df¼d. There-\\nfore, for purely categorical data, the object-cluster similarity is\\ncalculated with\\nsðxi,CjÞ¼Xd\\nr¼1HArPd\\nt¼1HAt/C1sAr¼xirðCjÞ\\nsAraNULLðCjÞ !\\n: ð20Þ\\nBy contrast, when clustering analysis is conducted on purely\\nnumerical data, with dc¼0 and df¼1, the deﬁned object-cluster\\nsimilarity metric will degenerate to\\nsðxi,CjÞ¼expð/C00:5Disðxi,cjÞÞ\\nPk\\nt¼1expð/C00:5Disðxi,ctÞÞ: ð21ÞY.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2231'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 4}, page_content='4. Iterative clustering algorithm\\nIn this section, we will present an iterative clustering algo-\\nrithm based on the proposed object-cluster similarity metric to\\nconduct clustering analysis.\\nThis paper concentrates on hard partition only, i.e., qijAf0,1g,\\nalthough it can be easily extended to the soft partition in terms of\\nposterior probability. Under the circumstances, given a set of N\\nobjects, the optimal Qn¼fqn\\nijgin Eq. (1)can be given by\\nqn\\nij¼1i f sðxi,CjÞZsðxi,CrÞ81rrrk,\\n0 otherwise ,(\\nð22Þ\\nwhere i¼1,2,... ,Nand j¼1,2,... ,k. That is, each object xiwill be\\nassigned to the cluster that has the largest object-cluster similarity\\nwith it among the kclusters. Therefore, an iterative algorithm can\\nbe conducted as Algorithm 1 to implement the clustering analysis.\\nAlgorithm 1. Iterative clustering learning based on object-cluster\\nsimilarity metric (OCIL).\\nInput: data set X¼fx1,x2,... ,xNg, number of clusters k\\nOutput: cluster label Y¼fy1,y2,... ,yNg\\n1. Calculate the importance of each categorical attributeaccording to Eq. (11), if applicable\\n2. Set Y¼f0,0,... ,0gand select kinitial objects, one for each\\ncluster\\nrepeat Initialize noChange ¼true.\\nfori¼1toNdo\\n3.y\\nðnew Þ\\ni¼argmaxjAf1,...,kg½sðxi,CjÞ/C138\\nifyðnew Þ\\niayðoldÞ\\nithen\\nnoChange ¼false\\n4. Update the information of clusters Cyðnew Þ\\niand CyðoldÞ\\ni,\\nincluding the frequency of each categorical value and\\nthe centroid of numerical vectors.\\nend if\\nend for\\nuntil noChange istrue\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Initialization\\nSeed points\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 1\\nError\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 2\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Trace of numerical centerClass 1\\nClass 2\\nClass 1\\nClass 2Class 1\\nClass 2\\nFig. 1. Illustration of OCIL algorithm on synthetic data.In step 3 of Algorithm 1 (also called OCIL algorithm herein-\\nafter), the object-cluster similarity sðxi,CjÞis calculated with Eqs.\\n(19), (20) or (21) for mixed, categorical, or numerical data,\\nrespectively. Additionally, in order to update the cluster informa-\\ntion conveniently in step 4, two auxiliary matrices for each\\ncluster are maintained. One matrix is to record the frequency of\\neach categorical value occurring in this cluster, and the other\\nmatrix stores the mean vector of the numerical parts of all\\nobjects belonging to this cluster. Moreover, like the existing\\nclustering algorithms with similar framework, the positions of\\ninitialized kseed points in step 2 will somewhat inﬂuence the\\nﬁnal clustering accuracy. In the literature, different initialization\\nmethods for clustering performance improvement have been\\npresented, such as Refs. [34,35] for numerical data clustering\\nand Refs. [25,26] for categorical data clustering. However, to the\\nbest of our knowledge, such initialization reﬁnement for mixed\\ndata clustering has not been studied yet. As the studies of this\\nissue have been beyond the scope of this paper, we shall therefore\\nutilize the random initialization method with multiple repetition\\nto get the statistic information for clustering performance\\nevaluation.\\nTo illustrate the learning process of OCIL algorithm, we have\\ngenerated a set of three-dimensional mixed data for clustering\\nanalysis as shown in Fig. 1 (a). The different point patterns stand\\nfor the two categorical values and the numerical values are\\nrandomly distributed in the space of ½1,5/C138/C2½1,3/C138. Specially, we\\nhave selected two points which are very close to each other as the\\nseed points of the two clusters. After one learning epoch, i.e., a\\nscan of the whole data set, the obtained cluster membership by\\nOCIL has been visualized in Fig. 1 (b). It can be seen that most data\\npoints have been assigned to a reasonable cluster except four\\npoints. Subsequently, during the second learning epoch, these\\ninaccurate points are reassigned and the ﬁnal result is obtained.\\nFurthermore, in the clustering space, we have drawn the moving\\ntrace for the center of all numerical vectors in each cluster as\\nshown in Fig. 1 (d). These traces give us a visual description about\\nthe change of members in the two clusters during the iterative\\nlearning of OCIL algorithm.Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2232'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 5}, page_content='In practice, when Algorithm 1 is applied to purely numerical\\ndata and Euclidean distance is utilized to calculate Disðxi,cjÞ,\\naccording to the similarity metric deﬁned by Eq. (21), we can get\\nsðxi,CjÞZsðxi,CrÞ3expð/C00:5Jxi/C0cjJ2Þ\\nPk\\nt¼1expð/C00:5Jxi/C0ctJ2Þ\\nZexpð/C00:5Jxi/C0crJ2Þ\\nPk\\nt¼1expð/C00:5Jxi/C0ctJ2Þ\\n3expð/C00:5Jxi/C0cjJ2ÞZexpð/C00:5Jxi/C0crJ2Þ\\n3Jxi/C0cjJ2rJxi/C0crJ2, ð23Þ\\nwhere ‘‘3’’ means ‘‘equivalent to’’. Then, the clustering criterion\\nformulated by Eq. (22) can be simpliﬁed as\\nqn\\nij¼1i f Jxi/C0cjJ2rJxi/C0crJ281rrrk,\\n0 otherwise :(\\nð24Þ\\nThat is, each object will be assigned to the cluster whose centroid\\nis closest to it. Under the circumstances, the proposed algorithm\\nis equivalent to the k-means method.\\nNext, we further give the time complexity analysis of OCIL\\nalgorithm. It can be observed that the computation cost of step\\n1i sOðmNd cÞ. For each iteration, the cost of the ‘‘ for’’ statement is\\nOðmNkd cþNkd uÞ, where mis the average number of different\\nvalues that can be chosen by each categorical attribute. Therefore,\\nthe total time cost of this algorithm is OðtðmNkd cþNkd uÞÞ, where t\\nis the number of iterations. From the practical viewpoint, we\\noften have k5N,m5Nand t5N. Subsequently, the time com-\\nplexity of this algorithm is O(dN). Hence, the proposed algorithm\\nis efﬁcient for data clustering, particularly for a large data set.\\n5. Automatic selection of cluster number\\nSimilar to the k-prototype [12] and k-modes [8,27] algorithms,\\nthe OCIL algorithm proposed in Section 4 still suffers from a\\nselection problem of cluster number. That is, the cluster number k\\nshould be preassigned exactly equal to the true one; otherwise,\\nOCIL will lead to an incorrect clustering result. To overcome this\\nproblem, in the following, we further present a penalized compe-\\ntitive clustering algorithm based on the object-cluster similarity\\nmetric. The competition and penalization mechanisms in this\\nimproved method will enable it to do automatic cluster number\\nselection by gradually eliminating the redundant clusters.\\n5.1. Competition mechanism\\nSuppose Nobjects come from knunknown clusters. Initially,\\nwe set kðkZknÞclusters fC1,C2,... ,Ckg, and assign one object as a\\nseed point to each cluster. According to the competitive learning\\nproposed in [36], given an input xieach time, the winner Cv\\namong the kclusters is determined by the dissimilarity between\\nxiand each cluster as well as the winning frequency of this cluster\\nin the past. In our proposed method, the newly deﬁned object-\\ncluster similarity sðxi,CjÞis utilized to estimate the similarity\\nbetween an object and a cluster. Since the value of sðxi,CjÞfalls\\ninto the interval ½0,1/C138, we can evaluate the dissimilarity between\\nobject xiand a cluster Cjwith ð1/C0sðxi,CjÞÞ. Then, the clustering\\ntask based on the object-cluster similarity metric formulated byEq.(1)can be rewritten as\\nQ\\nn¼arg min\\nQXk\\nj¼1XN\\ni¼1qijð1/C0sðxi,CjÞÞ2\\n43\\n5: ð25ÞActually, Qnobtained from Eq. (25) is equal to that obtained from\\nEq.(1), because we have\\narg min\\nQXk\\nj¼1XN\\ni¼1qijð1/C0sðxi,CjÞÞ2\\n43\\n53arg min\\nQXk\\nj¼1XN\\ni¼1ðqij/C0qijsðxi,CjÞÞ2\\n43\\n5\\n3arg min\\nQXk\\nj¼1XN\\ni¼1qij/C0Xk\\nj¼1XN\\ni¼1qijsðxi,CjÞ2\\n43\\n5\\n3arg min\\nQN/C0Xk\\nj¼1XN\\ni¼1qijsðxi,CjÞ2\\n43\\n5\\n3arg max\\nQXk\\nj¼1XN\\ni¼1qijsðxi,CjÞ2\\n43\\n5: ð26Þ\\nSubsequently, analogous to [36], given an object xieach time,\\nthe winner Cvamong the kclusters is determined by\\nv¼arg min\\n1rjrk½gjð1/C0sðxi,CjÞÞ/C138 , ð27Þ\\nwith the relative winning frequency gjofCjdeﬁned as\\ngj¼njPk\\nt¼1nt, ð28Þ\\nwhere njis the winning times of Cjin the past. That is, the winning\\nchance of a cluster is controlled by the object-cluster similarity as\\nwell as its winning frequency in the past competitions. Here,\\nreducing the winning rate of frequent winners is to solve the\\ndead-unit problem encountered by competitive learning [36].\\nAfter selecting out the winning cluster Cv, we assign xito it and\\nupdate the statistic information of Cv, which includes the center\\nof numerical part cvand the frequency of each categorical value\\naccompanying with xiinCv. Meanwhile, the winning times of Cv\\nis adjusted by\\nnðnew Þ\\nv ¼nðoldÞ\\nvþ1: ð29Þ\\nTherefore, a competitive learning version of the OCIL algorithm\\ncan be summarized as Algorithm 2 .\\nAlgorithm 2. Competitive learning based on object-cluster simi-\\nlarity metric (CL-OC).\\nInput: data set X, number of clusters k\\nOutput: cluster label Y¼fy1,y2,... ,yNg\\n1. Select kinitial objects, one for each cluster, and set\\nY¼f0,0,... ,0g,nj¼1 for j¼1,2,... ,k.\\nrepeat\\nInitialize noChange ¼true.\\nfori¼1toNdo\\n2.v¼arg min 1rjrk½gjð1/C0sðxi,CjÞÞ/C138\\n3. Let yðnew Þ\\ni¼v,nðnew Þ\\nv ¼nðoldÞ\\nvþ1, and update the statistic\\ninformation of Cvbased on xi.\\nifyðnew Þ\\niayðoldÞ\\nithen\\nnoChange ¼false\\nend if\\nend for\\nuntil noChange istrue\\n5.2. Penalization mechanism\\nIt has been demonstrated in [9,11] that the penalization\\nmechanism can enable the clustering algorithm to select the\\ncluster number automatically during the learning process by\\ngradually fading out the redundant clusters. Hence, in this paper,Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2233'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 6}, page_content='we also utilize this mechanism to solve the selection problem of\\ncluster number.\\nThe basic idea of the penalization mechanism is that, for each\\ninput xi, not only the winning cluster is updated based on xi, but\\nalso the rival nearest to the winner (i.e., the runner-up) is\\npenalized according to a speciﬁc criterion. Generally, in this kind\\nof method, the cluster number kis initialized not less than the\\ntrue one (i.e., kZkn) and the main task is to fade out the\\nredundant clusters. Therefore, in our approach, a weight is\\nassigned to each cluster. This weight is utilized to measure the\\nimportance of each cluster to the whole cluster structure. Speci-\\nﬁcally, all clusters with an equal weight means that each of them\\nhas the same contribution to the cluster structure. In case a\\ncluster has a very low weight, then the number of objects\\nassigned to it will decrease and ﬁnally this cluster will be\\neliminated. Subsequently, similar to Eq. (27), given an object xi\\neach time, the winner Cvamong kclusters satisﬁes\\nv¼arg min\\n1rjrk½gjð1/C0ljsðxi,CjÞÞ/C138 , ð30Þ\\nand its nearest rival Cris determined by\\nr¼arg min\\njav½gjð1/C0ljsðxi,CjÞÞ/C138 , ð31Þ\\nwhere ljis the weight of cluster Cjand the similarity between xi\\nand Cjis further regulated by it.\\nAfter selecting out the winning cluster and its nearest rival, on\\nthe one hand, we assign xito the winner Cvand update the\\nstatistic information of this cluster as well as its winning times.\\nOn the other hand, we further reward the winner by increasing its\\nweight according to\\nlðnew Þ\\nv ¼lðoldÞ\\nvþZ, ð32Þ\\nand meanwhile penalize the nearest rival Crby decreasing its\\nweight with\\nlðnew Þ\\nr ¼max ð0,lðoldÞ\\nr/C0Zsðxi,CrÞÞ, ð33Þ\\nwhere Zis a small learning rate and the ‘‘max ðÞ’’ function is to\\nmake sure that all the cluster weights are nonnegative. From\\nEq.(33), we can see that the rival-penalized strength increaseswith the similarity between xiand the rival. Consequently, the\\nmain steps of the penalized competitive leaning based on the\\nobject-cluster similarity can be summarized as Algorithm 3 .\\nAlgorithm 3. Penalized competitive learning based on object-\\ncluster similarity metric (PCL-OC).\\nInput: data set X, learning rate Zand a initial value of k(kZkn)\\nOutput: cluster label Y¼fy1,y2,... ,yNgand cluster number kn\\n1. Select kinitial objects, one for each cluster, and set\\nY¼f0,0,... ,0g,nj¼1 and lj¼1 for j¼1,2,... ,k.\\nrepeat\\nInitialize noChange ¼true.\\nfori¼1toNdo\\n2. Determine vand raccording to Eqs. (30) and (31) ,\\nrespectively.\\n3. Let yðnew Þ\\ni¼v,nðnew Þ\\nv ¼nðoldÞ\\nvþ1, and update the statistic\\ninformation of Cvbased on xi.\\n4. Update lvandlrusing Eqs. (32) and (33) , respectively.\\nifyðnew Þ\\niayðoldÞ\\nithen\\nnoChange ¼false\\nend if\\nend for\\nuntil noChange istrue\\nAfter the clustering learning using Algorithm 3 (also called PCL-\\nOC algorithm hereinafter), if there exists a cluster to which no\\nobjects belong, it will be regarded as a redundant one and simply\\nneglected. In Fig. 2 , we have visualized the learning process of PCL-\\nOC algorithm with the same synthetic data set that was utilized in\\nSection 4 . Initially, the cluster number was set at three and an\\nequal weight 1.0 was assigned to each cluster. Since this data set is\\nvery simple, the optional value range of learning rate Zis relatively\\nlarge. In our illustration, we have set Z¼0:01 to get a fast\\nconvergence speed. After one learning epoch, we can ﬁnd that\\nthe weight of the second cluster has decreased because it has\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Initialization\\nSeed points\\nλ1 = 1.0, λ2 = 1.0, λ3 = 1.0.\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 1\\nλ1 = 1.1901, λ2 = 0.9828, λ3 = 1.4871.\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 2\\nλ1 = 1.5414, λ2 = 0.7843, λ3 = 1.9871.\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 4\\nClass 1\\nClass 3\\nλ1 = 2.5415, λ2 = 0.0628, λ3 = 2.9871.Class 1\\nClass 2\\nClass 3\\nClass 1\\nClass 2\\nClass 3\\nFig. 2. Illustration of PCL-OC algorithm on synthetic data.Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2234'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 7}, page_content='suffered the most penalization and gained the least cluster mem-\\nbers. This penalization has been transmitted and strengthened\\nduring the following iterations and after the fourth epoch, no data\\npoints were assigned to the second cluster due to its low weight. If\\nthe iteration continues, one more epoch later we can get l2¼0,\\nwhich indicates that this cluster has been totally eliminated from\\nthe hypothetic cluster model.\\n6. Experiments\\nThis section investigates the effectiveness of the proposed\\napproaches for data clustering. We applied them to various data\\nsets obtained from UCI Machine Learning Data Repository (URL:\\nhttp://archive.ics.uci.edu/ml/ ) and compared their performance\\nwith the existing counterparts. In the experiments, the clustering\\naccuracy [37] was estimated by\\nACC ¼PN\\ni¼1dðci,map ðliÞÞ\\nN,\\nwhere Nis the number of instances in the data set, cistands for\\nthe provided label, map ðliÞis a mapping function which maps the\\nobtained cluster label lito the equivalent label from the data\\ncorpus, and the delta function dðci,map ðliÞÞ ¼1 only if ci¼map ðliÞ,\\notherwise 0. Correspondingly, the clustering error rate is com-\\nputed as e¼1/C0ACC. The algorithms were coded with MATLAB\\nand all the experiments were implemented by a desktop PC\\ncomputer with Intel(R) Core(TM)2 Quad CPU, 2.40 GHz main\\nfrequency, and 4 GB DDR2 667 RAM.\\n6.1. Performance evaluation of OCIL algorithm\\nIn this part, we experimentally investigated the performance\\nof proposed OCIL algorithm. For comparative studies, the results\\nof OCIL algorithm have been compared with k-means [4],\\nk-prototype [12], and k-modes [8,27] algorithms because of two\\nreasons: on the one hand, these algorithms all have the same time\\ncomplexity: i.e., O(dN). On the other hand, they have similar\\nframework and procedure so that the effectiveness of the pro-\\nposed similarity metric can be well evaluated. Please note that the\\nOCIL algorithm is equivalent to the k-means algorithm as shown\\ninSection 4 if the data have the numerical attributes only. Under\\nthe circumstances, the effectiveness of OCIL algorithm on numer-\\nical data set therefore becomes transparent and will not be\\ninvestigated any more. In the following sub-sections, we shall\\nfocus on investigating the clustering performance of OCIL on\\nmixed and categorical data sets, respectively.\\n6.1.1. Experiments on mixed data sets\\nFirstly, we investigated the performance of OCIL algorithm on\\nmixed data. The information of the selected data sets is shown in\\nTable 1 . The performance of OCIL method on mixed data has been\\ncompared with k-prototype algorithm and k-means algorithm.\\nEach algorithm has been executed 100 times on each data set and\\nthe clustering results are statistically summarized in Table 2 .I n\\nk-prototype method, the distance regulation parameter gwas set\\nat 0 :5s, where sis the average standard deviation of numerical\\nattributes. As for k-means, we utilized the single number repre-\\nsentation method to covert categorical attributes into numerical\\nones. That is, each categorical value was represented by an\\narbitrarily chosen integer. Additionally, Euclidean distance metric\\nwas adopted to estimate the distances between numerical vectorsin each method.\\nFrom Table 2 , it can be observed that, with random initializa-\\ntions, the proposed parameter-free algorithm OCIL outperforms\\nthe k-prototype and k-means methods in terms of clusteringaccuracy. Further, as shown in Table 1 , the ratios of categorical\\nattributes to numerical attributes in the utilized data sets are\\ndifferent from each other, especially the Dermatology data, which\\nhas only one numerical feature but 33 categorical ones. Never-\\ntheless, the OCIL has achieved a satisfactory clustering result. This\\nindicates that the proposed object-cluster similarity metric is\\napplicable to data in variant compound styles without using any\\nparameter to adjust between categorical and numerical attri-\\nbutes. Additionally, for the last three data sets (i.e., German,\\nAdult, and Dermatology) which have very uneven class distribu-\\ntions, the OCIL algorithm can give much improved accuracies\\ncompared to the other two methods. This result shows that, in\\ncomparison with numerically representing the distance between\\ncategorical values, the presented similarity metric in this paper is\\na more reasonable measurement for cluster analysis on mixed\\ndata and can well reveal the inherent cluster membership for\\neither heterogeneous or homogeneous clusters. Moreover, com-\\nparing the average running time of OCIL and k-prototypeTable 1\\nStatistics of the mixed data sets.\\nData set Instance Attribute ( dcþdu)Class Class probabilities\\nStatlog heart 270 7 þ6 2 55.56% 44.44%\\nHeart disease 303 7 þ6 2 54.13% 45.87%\\nCredit approval 653 9 þ6 2 54.67% 45.33%\\nGerman credit 1000 13 þ7 2 70.0% 30.0%\\nAdult 30,162 8 þ6 2 75.11% 24.89%\\nDermatology 366 33 þ1 6 30.6% 16.67% 19.67%\\n13.39% 14.21% 5.46%\\nTable 2Clustering errors of OCIL on mixed data sets in comparison with k-prototype and\\nk-means.\\nData set k-Means k-Prototype OCIL\\nStatlog 0.4047 70.0071 0.2306 70.0821 0.176170.0059\\nHeart 0.4224 70.0131 0.2280 70.0903 0.168770.0033\\nCredit 0.4487 70.0016 0.261970.0976 0.243770.0866\\nGerman 0.3290 70.0014 0.3289 70.0006 0.3057 70.0009\\nAdult 0.3869 70.0067 0.3855 70.0143 0.249070.0001\\nDermatology 0.7006 70.0216 0.690370.0255 0.302670.0973\\nTable 3\\nComparison of the average convergence time between k-prototype and OCIL.\\nData set k-Prototype (s) OCIL (s)\\nStatlog 0.0519 0.0498\\nHeart 0.0639 0.0491\\nCredit 0.1323 0.1282\\nGerman 0.2999 0.3342\\nAdult 15.2795 3.5447\\nDermatology 0.3674 0.1811\\nTable 4\\nStatistics of the categorical data sets.\\nData set Instance Attribute Class Class probabilities\\nSoybean 47 35 4 21.28% 21.28% 21.28% 36.16%\\nBreast 699 9 2 65.52% 34.48%\\nVote 435 16 2 61.38% 38.62%\\nZoo 101 16 7 40.59% 19.8% 4.95% 12.87% 3.97%\\n7.92% 9.9%Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2235'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 8}, page_content='algorithms listed in Table 3 , we can ﬁnd that the total running\\ntime of OCIL is no more than the one of k-prototype although OCIL\\nneeds additional time to calculate the weight of each categorical\\nattribute. That is because OCIL converges faster than k-prototype\\nin most cases.\\n6.1.2. Experiments on categorical data sets\\nNext, we further investigated the performance of OCIL algo-\\nrithm on purely categorical data. The information of utilized four\\ndifferent benchmark data sets has been summarized in Table 4 .T o\\nconduct comparison study, we have also implemented the other\\ntwo existing categorical data clustering algorithms: original\\nk-modes (H’s k-modes) [8]and k-modes with Ng’s dissimilarity\\nmetric (N’s k-modes) [27].\\nIn the experiment, each algorithm was conducted with ran-\\ndom initializations. Table 5 lists the average value and standard\\ndeviation in error obtained by OCIL and the other two algorithms,\\nrespectively. It can be seen that, for categorical data learning, the\\nproposed clustering method has competitive advantage in terms\\nof clustering accuracy and robustness compared with the other\\ntwo methods. This superiority of OCIL method mainly owes to\\ntwo merits of the object-cluster similarity metric. The ﬁrst one is\\nthat, in the proposed metric, the similarity between given\\ncategorical attribute value and a cluster depends on the distribu-\\ntion of this value within the cluster, but not the numerical\\ndistance between this value and the corresponding attribute\\nvalue of the cluster mode. In N’s k-modes, when calculating the\\ndistance between an object and a cluster mode, the frequencies of\\nattribute values within the cluster are considered if the object and\\ncluster mode have the same values. Hence, the performance of N’sk-modes is better than H’s k-modes on all the data sets we have\\ntried so far. However, when the object and cluster mode have\\ndifferent attribute values, N’s k-modes also simply assumes the\\ndistance is 1. The other merit is that we do not utilize mode to\\nrepresent each cluster but calculate the similarity based on the\\ncluster’s statistic information in this new metric. In k-modes\\nalgorithms, a cluster mode is represented by the most frequent\\nattribute values within the cluster. That is, only one value is\\nselected as the representation for each attribute even though\\nthere may be some value with proximate frequency. Hence, the\\ninformation of a cluster actually cannot be completely presented\\nby the deﬁned mode for categorical data.\\nAdditionally, we further evaluated the convergence speed of\\nthe proposed method on categorical data clustering. Table 6 lists\\nthe average convergence time over 100 runs cost by each method.It can be observed that the convergence time of the proposed\\nmethod is much faster than the k-modes with the improvement\\nof 60% on average in all cases we have tried so far. Based on the\\nanalysis of experimental results, the signiﬁcant advantage of\\nrunning time with OCIL algorithm on categorical data can be\\nowed to the following two aspects: on the one hand, the\\nconvergence speed of OCIL is faster than k-modes as the number\\nof learning epoches needed by OCIL is smaller than that needed\\nbyk-modes on average; on the other hand, the computational\\ncost of OCIL in each learning epoch is less than the k-modes\\nbecause k-modes needs to update the cluster modes in each\\nlearning step while the OCIL need not.\\n6.2. Performance evaluation of PCL-OC algorithm\\nTo investigate the effectiveness of the proposed penalized\\ncompetitive learning method, we have applied it to different real\\ndata sets, including purely categorical data and mixed data.\\nMoreover, to the best of our knowledge, clustering algorithmwith automatic cluster number selection for mixed data has not\\nbeen studied yet in the literature. Therefore, in our experiments,\\nwe only take the k-prototype algorithm as an example to\\ncomparatively show the outstanding performance of PCL-OC that\\nis capable of determining the number of clusters automatically.\\nAs a rule of thumb, the learning rate\\nZin the penalization\\nmechanism can be set as rðk=NÞ, where ris a small coefﬁcient\\nand Nis the number of objects in the given data set. That is, the\\noptimal learning rate increases with the initial cluster number k\\nbut decreases with N. The value of ralso has small variation for\\ndifferent size of data set. Generally, a too small value of rwill lead\\nto an insufﬁcient penalization process and the redundant clusters\\ncannot be completely driven out from the input space. Conver-\\nsely, a too large value of rwill cause an excessive penalization,\\nwhereby the initial clusters will be over-eliminated. By the rule of\\nthumb, it is appropriate to set the value of rbetween 0.001 and\\n0.003 for Soybean data that contains 47 instances only. For other\\ndata sets with hundreds of instances, rcan be set between 0.003\\nand 0.006. In the following two experiments, the value of ris set\\nat 0.001 and 0.005, respectively.\\nI nt h eﬁ r s te x p e r i m e n t ,w et o o kt h eV o t ed a t as e tf o ri n s t a n c et o\\nshow PCL-OC algorithm’s ability of automatical cluster number\\nselection on real data set. To show the details of learning process,\\nwe utilized ajðtÞto record the proportion of objects among the whole\\ndata set that has been assigned to the jth cluster during the tth\\nTable 5\\nComparison of the clustering errors obtained by three different methods oncategorical data sets.\\nData set H’s k-modes N’s k-modes OCIL\\nSoybean 0.1691 70.1521 0.096470.1404 0.1017 70.1380\\nBreast 0.1655 70.1528 0.1356 70.0016 0.093470.0009\\nVote 0.1387 70.0066 0.1345 70.0031 0.121370.0010\\nZoo 0.2873 70.1083 0.2730 70.0818 0.2681 70.0906\\nTable 6\\nComparison of the average convergence time between k-modes and OCIL.\\nData set H’s k-modes (s) N’s k-modes (s) OCIL (s)\\nSoybean 0.0176 0.0189 0.0058\\nBreast 0.1044 0.1515 0.0540\\nVote 0.0733 0.0862 0.0354\\nZoo 0.0418 0.0514 0.00980 50 100 150 200 250 30000.10.20.30.40.50.60.7Vote Data with k = 5\\nNo. of learning epochesValues of αjs\\nFig. 3. Learning curves of ajs obtained by PCL-OC on Vote data with k¼5.Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2236'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 9}, page_content='learning epoch. Initially, ﬁve clusters were generated and the seed\\npoints were randomly selected in the input space. The learning curves\\nofajs over the epoches obtained by the PCL-OC algorithm are shown\\ninFig. 3 . It can be seen that the values of three ajsh a v ec o n v e r g e dt o\\naround zero after about 150 learni ng epoches. It means that these\\nthree redundant clusters have been eliminated from the whole\\nclustering structure because few objects will be assigned to them.\\nMeanwhile, the obtained values of the other two ajs are 0.6069 and\\n0.3862, which are approximate to the proportions of the two true\\nclusters in the data set. Hence, the PCL-OC algorithm has successfully\\nidentiﬁed the true cluster number during the learning process.\\nIn the second experiment, we investigated the performance of\\nPCL-OC on different data sets with variant settings of k.I nt o t a l ,\\nfour data sets were utilized: two mixed data sets with numericaland categorical attributes and two with purely categorical attri-\\nbutes. These data sets have different cluster numbers and class\\ndistributions. For each data set, the PCL-OC has been executed 50\\ntimes and the learning results are summarized in Table 7 .I tc a nb e\\nseen that the PCL-OC algorithm can give a good estimation of the\\ncluster number in each setting of k. For comparison, we have also\\nimplemented k-prototype algorithm [12] under the same environ-\\nment as PCL-OC. Evidently, the k-prototype algorithm needs to pre-\\nassign the number of clusters exactly without the capability of\\nselecting the cluster number automatically. As a result, the\\nclustering accuracy of the k-prototype is seriously degraded when\\nthe number kof clusters is not selected appropriately in advance.\\n7. Conclusion\\nIn this paper, we have proposed a general clustering framework\\nbased on object-cluster similarity, through which a uniﬁed similarity\\nmetric for both categorical and numerical attributes has been\\npresented. Under this new metric, t he object-cluster similarity for\\ncategorical and numerical attributes are with the same scale, which\\nis beneﬁcial to clustering analysis on various data types. Subse-\\nquently, an iterative algorithm has been introduced to implement\\nthe data clustering. The advantages of the proposed method have\\nbeen experimentally demonstrated in comparison with the existing\\ncounterparts. Additionally, to o vercome the cluster number selec-\\ntion problem, a penalized competitive learning algorithm has beenpresented within the proposed clus tering framework. The competi-\\ntion and penalization mechanisms embedded in this method are\\ncapable of selecting number of clusters automatically by gradually\\nfading out the redundant clusters during the clustering process.\\nExperiments on different benchmark data sets have shown the\\neffectiveness and efﬁciency of the proposed approach.Conﬂict of interest statement\\nNone declared.\\nAcknowledgments\\nThe work described in this paper was supported by the Faculty\\nResearch Grant of Hong Kong Baptist University with the Project\\nCode: FRG2/11-12/067, and the NSFC under grant 61272366.\\nReferences\\n[1] R.S. Michalski, I. Bratko, M. Kubat, Machine Learning and Data Mining:\\nMethods and Applications, Wiley, New York, 1998.\\n[2] W. Cai, S. Chen, D. Zhang, Fast and robust fuzzy c-means clustering\\nalgorithms incorporating local information for image segmentation, PatternRecognition 40 (3) (2007) 825–838.\\n[3] A.W.-C. Liew, H. Yan, M. Yang, Pattern recognition techniques for the\\nemerging ﬁeld of bioinformatics: a review, Pattern Recognition 38 (11)\\n(2005) 2055–2073.\\n[4] J.B. MacQueen, Some methods for classiﬁcation and analysis of multivariate\\nobservations, in: Proceedings of the Fifth Berkeley Symposium on Mathema-\\ntical Statistics and Probability, University of California Press, 1967, 281–297.\\n[5] A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete\\ndata via the EM algorithm, Journal of the Royal Statistical Society, Series B\\n(Methodological) 39 (1) (1977) 1–38.\\n[6] C.C. Hsu, Generalizing self-organizing map for categorical data, IEEE Transac-\\ntions on Neural Networks 17 (2) (2006) 294–304.\\n[7] C. Li, G. Biswas, Unsupervised learning with mixed numeric and nominal data,\\nIEEE Transactions on Knowledge and Data Engineering 14 (4) (2002) 673–690.\\n[8] Z. Huang, A fast clustering algorithm to cluster very large categorical data\\nsets in data mining, in: Proceedings of the SIGMOD Workshop on Research\\nIssues on Data Mining and Knowledge Discovery, 1997, pp. 1–8.\\n[9] L. Xu, A. Krzyzak, E. Oja, Rival penalized competitive learning for clustering\\nanalysis, RBF net, and curve detection, IEEE Transactions on Neural Networks\\n4 (4) (1993) 636–648.\\n[10] H. Liao, M.K. Ng, Categorical data clustering with automatic selection of\\ncluster number, Fuzzy Information and Engineering 1 (1) (2009) 5–25.\\n[11] Y.M. Cheung, Maximum weighted likelihood via rival penalized EM for\\ndensity mixture clustering with automatic model selection, IEEE Transactions\\non Knowledge and Data Engineering 17 (6) (2005) 750–761.\\n[12] Z. Huang, Clustering large data sets with mixed numeric and categorical\\nvalues, in: Proceedings of the First Paciﬁc-Asia Conference on Knowledge\\nDiscovery and Data Mining, 1997, pp. 21–34.\\n[13] S. Guha, R. Rastogi, K. Shim, ROCK: a robust clustering algorithm for\\ncategorical attributes, Information Systems 25 (5) (2001) 345–366.\\n[14] E. Cesario, G. Manco, R. Ortale, Top-down parameter-free clustering of high-\\ndimensional categorical data, IEEE Transactions on Knowledge and Data\\nEngineering 19 (12) (2007) 1607–1624.\\n[15] M.J. Zaki, M. Peters, CLICK: mining subspace clusters in categorical data via k-\\npartite maximal cliques, in: Proceedings of the twenty-ﬁrst International\\nConference on Data Engineering, 2005, pp. 355–356.\\n[16] D. Barbara, J. Couto, Y. Li, COOLCAT: an entropy-based algorithm for\\ncategorical clustering, in: Proceedings of the 11th ACM Conference on\\nInformation and Knowledge Management, 2002, pp. 582–589.\\n[17] P. Andritsos, P. Tsaparas, R.J. Miller, K.C. Sevcik, LIMBO: scalable clustering of\\ncategorical data, in: Proceedings of the 9th International Conference onExtending Database Technology, 2004, pp. 123–146.\\n[18] N. Tishby, F.C. Pereira, W. Bialek, The information bottleneck method, in:\\nProceedings of the 37th Annual Allerton Conference on Communication,\\nControl and Computing, 1999, pp. 368–377.\\n[19] D.W. Goodall, A new similarity index based on probability, Biometric 22 (4)\\n(1966) 882–907.\\n[20] Z. He, X. Xu, S. Deng, Scalable algorithms for clustering large datasets with\\nmixed type attributes, International Journal of Intelligence Systems 20 (2005)\\n1077–1089.\\n[21] H. Luo, F. Kong, Y. Li, Clustering mixed data based on evidence accumulation,\\nin: X. Li, O. R. Zaiane, Z. Li (Eds.), Advanced Data Ming and Applications,\\nLecture Notes in Computer Science, vol. 4093, 2006, pp. 348–355.\\n[22] P. Cheeseman, J. Stutz, Bayesian classiﬁcation (AutoClass): theory and results,\\nin: Advances in Knowledge Discovery and Data Mining, 1996.\\n[23] Z. Huang, Extensions to the k-modes algorithm for clustering large data sets\\nwith categorical values, Data Mining and Knowledge Discovery 2 (3) (1998)\\n283–304.\\n[24] Z. Huang, M. Ng, A note on k-modes clustering, Journal of Classiﬁcation 20 (2)\\n(2003) 257–261.\\n[25] S.S. Khan, S. Kant, Computation of initial modes for k-modes clustering\\nalgorithm using evidence accumulation, in: Proceedings of the 20th Interna-\\ntional Joint Conference on Artiﬁcial Intelligence (IJCAI-07), 2007, pp. 2784–2789.\\n[26] F. Cao, J. Liang, L. Bai, A new initialization method for categorical data\\nclustering, Expert Systems with Applications 36 (7) (2009) 10223–10228.Table 7\\nClustering results of PCL-OC on different data sets with variant settings of k.\\nData set knk Cluster no. (Mean 7std) Error rate\\n(mean7std)\\nPCL-OC k-Prototype PCL-OC\\nHeart 2 3 1.7070.47 3.0 70.0 0.2315 70.1073\\n4 1.8070.41 4.0 70.0 0.2507 70.0752\\n5 2.2070.61 5.0 70.0 0.2458 70.1191\\nCredit 2 3 1.8670.36 3.0 70.0 0.2645 70.1063\\n4 2.1670.67 4.0 70.0 0.2609 70.1089\\n5 2.3470.82 5.0 70.0 0.2734 70.1048\\nSoybean 4 5 4.4270.50 5.0 70.0 0.0853 70.0790\\n6 4.1870.77 6.0 70.0 0.1106 70.1025\\n7 4.0471.14 7.0 70.0 0.1021 70.0932\\nVote 2 3 2.070.0 3.0 70.0 0.1196 70.0001\\n4 2.070.0 4.0 70.0 0.1196 70.0002\\n5 2.070.0 5.0 70.0 0.1198 70.0005Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2237'),\n",
       " Document(metadata={'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf', 'page': 10}, page_content='[27] M.K. Ng, M.J. Li, J.Z. Huang, Z. He, On the impact of dissimilarity measure in k-\\nmodes clustering algorithm, IEEE Transactions on Pattern Analysis and\\nMachine Intelligence 29 (3) (2007) 503–507.\\n[28] H. Akaike, A new look at the statistical model identiﬁcation, IEEE Transac-\\ntions on Automatic Control 19 (6) (1974) 716–723.\\n[29] G. Schwarz, Estimating the dimension of a model, The Annals of Statistics 6\\n(2) (1978) 461–464.\\n[30] Y.M. Cheung, A competitive and cooperative learning approach to robust data\\nclustering, in: Proceedings of IASTED International Conference on Neural\\nNetworks and Computational Intelligence, 2004, pp. 131–136.\\n[31] M. Filippone, F. Camastra, F. Masulli, S. Rovetta, A survey of kernel and\\nspectral methods for clustering, Pattern Recognition 41 (1) (2008) 176–190.\\n[32] A.K. Jain, Data clustering: 50 years beyond k-means, Pattern Recognition\\nLetters 31 (8) (2010) 651–666.[33] J. Basak, R. Krishnapuram, Interpretable hierarchical clustering by construct-\\ning an unsupervised decision tree, IEEE Transactions on Knowledge and DataEngineering 17 (1) (2005) 121–132.\\n[34] J.P. na, J. Lozano, P.L. naga, An empirical comparison of four initialization\\nmethods for the k-means algorithm, Pattern Recognition Letters 20 (10)\\n(1999) 1027–1040.\\n[35] S.S. Khan, A. Ahmad, Cluster center initialization algorithm for k-means\\nclustering, Pattern Recognition Letters 25 (11) (2004) 1293–1302.\\n[36] S.C. Ahalt, A.K. Krishnamurthy, P. Chen, D.E. Melton, Competitive learning\\nalgorithms for vector quantization, Neural Networks 3 (3) (1990) 277–290.\\n[37] X. He, D. Cai, P. Niyogi, Laplacian score for feature selection, in: Advances in\\nNeural Information Processing Systems, 2005.\\nYiu-ming Cheung (SM’06) is a professor at Department of Computer Science in Hong Kong Baptist University. He received the Ph.D. degree from the Department of\\nComputer Science and Engineering, The Chinese University of Hong Kong in 2000, and then joined the Department of Computer Science at Hong Kong Baptist University in\\n2001. His current research interests are in the ﬁelds of machine learning and information security, particularly the topics on clustering analysis, blind source separation,\\nneural networks, nonlinear optimization, watermarking and lip-reading. He is the founding chairman of IEEE (Hong Kong) Computational Intelligenc e Chapter. Currently,\\nhe is also the associate editor of Knowledge and Information Systems, as well as the guest co-editor and editorial board member of the several internat ional journals.\\nHong Jia received the B.S. and master degrees from Huazhong University of Science and Technology, China, in 2008 and 2010, respectively. She is currently the P h.D.\\nstudent at the Department of Computer Science in Hong Kong Baptist University, Hong Kong. Her research interests include clustering analysis and pat tern recognition.Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2238')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_loader = PyPDFLoader(\"/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf\")\n",
    "docs = document_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 4, 'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf'}, page_content='4. Iterative clustering algorithm\\nIn this section, we will present an iterative clustering algo-\\nrithm based on the proposed object-cluster similarity metric to\\nconduct clustering analysis.\\nThis paper concentrates on hard partition only, i.e., qijAf0,1g,\\nalthough it can be easily extended to the soft partition in terms of\\nposterior probability. Under the circumstances, given a set of N\\nobjects, the optimal Qn¼fqn\\nijgin Eq. (1)can be given by\\nqn\\nij¼1i f sðxi,CjÞZsðxi,CrÞ81rrrk,\\n0 otherwise ,(\\nð22Þ\\nwhere i¼1,2,... ,Nand j¼1,2,... ,k. That is, each object xiwill be\\nassigned to the cluster that has the largest object-cluster similarity\\nwith it among the kclusters. Therefore, an iterative algorithm can\\nbe conducted as Algorithm 1 to implement the clustering analysis.\\nAlgorithm 1. Iterative clustering learning based on object-cluster\\nsimilarity metric (OCIL).\\nInput: data set X¼fx1,x2,... ,xNg, number of clusters k\\nOutput: cluster label Y¼fy1,y2,... ,yNg\\n1. Calculate the importance of each categorical attributeaccording to Eq. (11), if applicable\\n2. Set Y¼f0,0,... ,0gand select kinitial objects, one for each\\ncluster\\nrepeat Initialize noChange ¼true.\\nfori¼1toNdo\\n3.y\\nðnew Þ\\ni¼argmaxjAf1,...,kg½sðxi,CjÞ/C138\\nifyðnew Þ\\niayðoldÞ\\nithen\\nnoChange ¼false\\n4. Update the information of clusters Cyðnew Þ\\niand CyðoldÞ\\ni,\\nincluding the frequency of each categorical value and\\nthe centroid of numerical vectors.\\nend if\\nend for\\nuntil noChange istrue\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Initialization\\nSeed points\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 1\\nError\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Epoch = 2\\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.50.511.522.533.5Trace of numerical centerClass 1\\nClass 2\\nClass 1\\nClass 2Class 1\\nClass 2\\nFig. 1. Illustration of OCIL algorithm on synthetic data.In step 3 of Algorithm 1 (also called OCIL algorithm herein-\\nafter), the object-cluster similarity sðxi,CjÞis calculated with Eqs.\\n(19), (20) or (21) for mixed, categorical, or numerical data,\\nrespectively. Additionally, in order to update the cluster informa-\\ntion conveniently in step 4, two auxiliary matrices for each\\ncluster are maintained. One matrix is to record the frequency of\\neach categorical value occurring in this cluster, and the other\\nmatrix stores the mean vector of the numerical parts of all\\nobjects belonging to this cluster. Moreover, like the existing\\nclustering algorithms with similar framework, the positions of\\ninitialized kseed points in step 2 will somewhat inﬂuence the\\nﬁnal clustering accuracy. In the literature, different initialization\\nmethods for clustering performance improvement have been\\npresented, such as Refs. [34,35] for numerical data clustering\\nand Refs. [25,26] for categorical data clustering. However, to the\\nbest of our knowledge, such initialization reﬁnement for mixed\\ndata clustering has not been studied yet. As the studies of this\\nissue have been beyond the scope of this paper, we shall therefore\\nutilize the random initialization method with multiple repetition\\nto get the statistic information for clustering performance\\nevaluation.\\nTo illustrate the learning process of OCIL algorithm, we have\\ngenerated a set of three-dimensional mixed data for clustering\\nanalysis as shown in Fig. 1 (a). The different point patterns stand\\nfor the two categorical values and the numerical values are\\nrandomly distributed in the space of ½1,5/C138/C2½1,3/C138. Specially, we\\nhave selected two points which are very close to each other as the\\nseed points of the two clusters. After one learning epoch, i.e., a\\nscan of the whole data set, the obtained cluster membership by\\nOCIL has been visualized in Fig. 1 (b). It can be seen that most data\\npoints have been assigned to a reasonable cluster except four\\npoints. Subsequently, during the second learning epoch, these\\ninaccurate points are reassigned and the ﬁnal result is obtained.\\nFurthermore, in the clustering space, we have drawn the moving\\ntrace for the center of all numerical vectors in each cluster as\\nshown in Fig. 1 (d). These traces give us a visual description about\\nthe change of members in the two clusters during the iterative\\nlearning of OCIL algorithm.Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2232'),\n",
       " Document(metadata={'page': 7, 'source': '/Users/toygunkarabas/Development/AI-Tutorials/test_cases/Categorical-and-numerical-attribute data clustering based on a unified similarity metric without knowing cluster number.pdf'}, page_content='suffered the most penalization and gained the least cluster mem-\\nbers. This penalization has been transmitted and strengthened\\nduring the following iterations and after the fourth epoch, no data\\npoints were assigned to the second cluster due to its low weight. If\\nthe iteration continues, one more epoch later we can get l2¼0,\\nwhich indicates that this cluster has been totally eliminated from\\nthe hypothetic cluster model.\\n6. Experiments\\nThis section investigates the effectiveness of the proposed\\napproaches for data clustering. We applied them to various data\\nsets obtained from UCI Machine Learning Data Repository (URL:\\nhttp://archive.ics.uci.edu/ml/ ) and compared their performance\\nwith the existing counterparts. In the experiments, the clustering\\naccuracy [37] was estimated by\\nACC ¼PN\\ni¼1dðci,map ðliÞÞ\\nN,\\nwhere Nis the number of instances in the data set, cistands for\\nthe provided label, map ðliÞis a mapping function which maps the\\nobtained cluster label lito the equivalent label from the data\\ncorpus, and the delta function dðci,map ðliÞÞ ¼1 only if ci¼map ðliÞ,\\notherwise 0. Correspondingly, the clustering error rate is com-\\nputed as e¼1/C0ACC. The algorithms were coded with MATLAB\\nand all the experiments were implemented by a desktop PC\\ncomputer with Intel(R) Core(TM)2 Quad CPU, 2.40 GHz main\\nfrequency, and 4 GB DDR2 667 RAM.\\n6.1. Performance evaluation of OCIL algorithm\\nIn this part, we experimentally investigated the performance\\nof proposed OCIL algorithm. For comparative studies, the results\\nof OCIL algorithm have been compared with k-means [4],\\nk-prototype [12], and k-modes [8,27] algorithms because of two\\nreasons: on the one hand, these algorithms all have the same time\\ncomplexity: i.e., O(dN). On the other hand, they have similar\\nframework and procedure so that the effectiveness of the pro-\\nposed similarity metric can be well evaluated. Please note that the\\nOCIL algorithm is equivalent to the k-means algorithm as shown\\ninSection 4 if the data have the numerical attributes only. Under\\nthe circumstances, the effectiveness of OCIL algorithm on numer-\\nical data set therefore becomes transparent and will not be\\ninvestigated any more. In the following sub-sections, we shall\\nfocus on investigating the clustering performance of OCIL on\\nmixed and categorical data sets, respectively.\\n6.1.1. Experiments on mixed data sets\\nFirstly, we investigated the performance of OCIL algorithm on\\nmixed data. The information of the selected data sets is shown in\\nTable 1 . The performance of OCIL method on mixed data has been\\ncompared with k-prototype algorithm and k-means algorithm.\\nEach algorithm has been executed 100 times on each data set and\\nthe clustering results are statistically summarized in Table 2 .I n\\nk-prototype method, the distance regulation parameter gwas set\\nat 0 :5s, where sis the average standard deviation of numerical\\nattributes. As for k-means, we utilized the single number repre-\\nsentation method to covert categorical attributes into numerical\\nones. That is, each categorical value was represented by an\\narbitrarily chosen integer. Additionally, Euclidean distance metric\\nwas adopted to estimate the distances between numerical vectorsin each method.\\nFrom Table 2 , it can be observed that, with random initializa-\\ntions, the proposed parameter-free algorithm OCIL outperforms\\nthe k-prototype and k-means methods in terms of clusteringaccuracy. Further, as shown in Table 1 , the ratios of categorical\\nattributes to numerical attributes in the utilized data sets are\\ndifferent from each other, especially the Dermatology data, which\\nhas only one numerical feature but 33 categorical ones. Never-\\ntheless, the OCIL has achieved a satisfactory clustering result. This\\nindicates that the proposed object-cluster similarity metric is\\napplicable to data in variant compound styles without using any\\nparameter to adjust between categorical and numerical attri-\\nbutes. Additionally, for the last three data sets (i.e., German,\\nAdult, and Dermatology) which have very uneven class distribu-\\ntions, the OCIL algorithm can give much improved accuracies\\ncompared to the other two methods. This result shows that, in\\ncomparison with numerically representing the distance between\\ncategorical values, the presented similarity metric in this paper is\\na more reasonable measurement for cluster analysis on mixed\\ndata and can well reveal the inherent cluster membership for\\neither heterogeneous or homogeneous clusters. Moreover, com-\\nparing the average running time of OCIL and k-prototypeTable 1\\nStatistics of the mixed data sets.\\nData set Instance Attribute ( dcþdu)Class Class probabilities\\nStatlog heart 270 7 þ6 2 55.56% 44.44%\\nHeart disease 303 7 þ6 2 54.13% 45.87%\\nCredit approval 653 9 þ6 2 54.67% 45.33%\\nGerman credit 1000 13 þ7 2 70.0% 30.0%\\nAdult 30,162 8 þ6 2 75.11% 24.89%\\nDermatology 366 33 þ1 6 30.6% 16.67% 19.67%\\n13.39% 14.21% 5.46%\\nTable 2Clustering errors of OCIL on mixed data sets in comparison with k-prototype and\\nk-means.\\nData set k-Means k-Prototype OCIL\\nStatlog 0.4047 70.0071 0.2306 70.0821 0.176170.0059\\nHeart 0.4224 70.0131 0.2280 70.0903 0.168770.0033\\nCredit 0.4487 70.0016 0.261970.0976 0.243770.0866\\nGerman 0.3290 70.0014 0.3289 70.0006 0.3057 70.0009\\nAdult 0.3869 70.0067 0.3855 70.0143 0.249070.0001\\nDermatology 0.7006 70.0216 0.690370.0255 0.302670.0973\\nTable 3\\nComparison of the average convergence time between k-prototype and OCIL.\\nData set k-Prototype (s) OCIL (s)\\nStatlog 0.0519 0.0498\\nHeart 0.0639 0.0491\\nCredit 0.1323 0.1282\\nGerman 0.2999 0.3342\\nAdult 15.2795 3.5447\\nDermatology 0.3674 0.1811\\nTable 4\\nStatistics of the categorical data sets.\\nData set Instance Attribute Class Class probabilities\\nSoybean 47 35 4 21.28% 21.28% 21.28% 36.16%\\nBreast 699 9 2 65.52% 34.48%\\nVote 435 16 2 61.38% 38.62%\\nZoo 101 16 7 40.59% 19.8% 4.95% 12.87% 3.97%\\n7.92% 9.9%Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2235')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "retriever.invoke(\"mixed data clustering algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"human\", template)])\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is an overview of the Mixed Data Clustering Algorithm:\n",
      "\n",
      "**Overview:**\n",
      "The Mixed Data Clustering Algorithm (OCIL) is a parameter-free algorithm that can handle mixed data sets containing both categorical and numerical attributes. It uses a novel similarity metric called object-cluster similarity to measure the similarity between objects in different clusters.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "1. **Parameter-Free:** The OCIL algorithm does not require any parameters to be set, making it easy to use.\n",
      "2. **Mixed Data Handling:** The algorithm can handle data sets with both categorical and numerical attributes.\n",
      "3. **Object-Cluster Similarity Metric:** A novel similarity metric that measures the similarity between objects in different clusters.\n",
      "\n",
      "**Comparison with Other Algorithms:**\n",
      "The OCIL algorithm was compared with k-means, k-prototype, and k-modes algorithms on mixed data sets. The results showed that OCIL outperformed these algorithms in terms of clustering accuracy.\n",
      "\n",
      "**Experimental Results:**\n",
      "\n",
      "1. **Clustering Accuracy:** The average clustering errors for the OCIL algorithm were lower than those of k-means, k-prototype, and k-modes.\n",
      "2. **Average Convergence Time:** The average convergence time for the OCIL algorithm was comparable to that of k-prototype.\n",
      "\n",
      "**Conclusion:**\n",
      "The Mixed Data Clustering Algorithm (OCIL) is a parameter-free algorithm that can handle mixed data sets with both categorical and numerical attributes. It uses a novel similarity metric called object-cluster similarity, which measures the similarity between objects in different clusters. The experimental results show that OCIL outperformed k-means, k-prototype, and k-modes algorithms on mixed data sets.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"tell me about mixed data clustering algorithm\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suffered the most penalization and gained the least cluster mem-\n",
      "bers. This penalization has been transmitted and strengthened\n",
      "during the following iterations and after the fourth epoch, no data\n",
      "points were assigned to the second cluster due to its low weight. If\n",
      "the iteration continues, one more epoch later we can get l2¼0,\n",
      "which indicates that this cluster has been totally eliminated from\n",
      "the hypothetic cluster model.\n",
      "6. Experiments\n",
      "This section investigates the effectiveness of the proposed\n",
      "approaches for data clustering. We applied them to various data\n",
      "sets obtained from UCI Machine Learning Data Repository (URL:\n",
      "http://archive.ics.uci.edu/ml/ ) and compared their performance\n",
      "with the existing counterparts. In the experiments, the clustering\n",
      "accuracy [37] was estimated by\n",
      "ACC ¼PN\n",
      "i¼1dðci,map ðliÞÞ\n",
      "N,\n",
      "where Nis the number of instances in the data set, cistands for\n",
      "the provided label, map ðliÞis a mapping function which maps the\n",
      "obtained cluster label lito the equivalent label from the data\n",
      "corpus, and the delta function dðci,map ðliÞÞ ¼1 only if ci¼map ðliÞ,\n",
      "otherwise 0. Correspondingly, the clustering error rate is com-\n",
      "puted as e¼1/C0ACC. The algorithms were coded with MATLAB\n",
      "and all the experiments were implemented by a desktop PC\n",
      "computer with Intel(R) Core(TM)2 Quad CPU, 2.40 GHz main\n",
      "frequency, and 4 GB DDR2 667 RAM.\n",
      "6.1. Performance evaluation of OCIL algorithm\n",
      "In this part, we experimentally investigated the performance\n",
      "of proposed OCIL algorithm. For comparative studies, the results\n",
      "of OCIL algorithm have been compared with k-means [4],\n",
      "k-prototype [12], and k-modes [8,27] algorithms because of two\n",
      "reasons: on the one hand, these algorithms all have the same time\n",
      "complexity: i.e., O(dN). On the other hand, they have similar\n",
      "framework and procedure so that the effectiveness of the pro-\n",
      "posed similarity metric can be well evaluated. Please note that the\n",
      "OCIL algorithm is equivalent to the k-means algorithm as shown\n",
      "inSection 4 if the data have the numerical attributes only. Under\n",
      "the circumstances, the effectiveness of OCIL algorithm on numer-\n",
      "ical data set therefore becomes transparent and will not be\n",
      "investigated any more. In the following sub-sections, we shall\n",
      "focus on investigating the clustering performance of OCIL on\n",
      "mixed and categorical data sets, respectively.\n",
      "6.1.1. Experiments on mixed data sets\n",
      "Firstly, we investigated the performance of OCIL algorithm on\n",
      "mixed data. The information of the selected data sets is shown in\n",
      "Table 1 . The performance of OCIL method on mixed data has been\n",
      "compared with k-prototype algorithm and k-means algorithm.\n",
      "Each algorithm has been executed 100 times on each data set and\n",
      "the clustering results are statistically summarized in Table 2 .I n\n",
      "k-prototype method, the distance regulation parameter gwas set\n",
      "at 0 :5s, where sis the average standard deviation of numerical\n",
      "attributes. As for k-means, we utilized the single number repre-\n",
      "sentation method to covert categorical attributes into numerical\n",
      "ones. That is, each categorical value was represented by an\n",
      "arbitrarily chosen integer. Additionally, Euclidean distance metric\n",
      "was adopted to estimate the distances between numerical vectorsin each method.\n",
      "From Table 2 , it can be observed that, with random initializa-\n",
      "tions, the proposed parameter-free algorithm OCIL outperforms\n",
      "the k-prototype and k-means methods in terms of clusteringaccuracy. Further, as shown in Table 1 , the ratios of categorical\n",
      "attributes to numerical attributes in the utilized data sets are\n",
      "different from each other, especially the Dermatology data, which\n",
      "has only one numerical feature but 33 categorical ones. Never-\n",
      "theless, the OCIL has achieved a satisfactory clustering result. This\n",
      "indicates that the proposed object-cluster similarity metric is\n",
      "applicable to data in variant compound styles without using any\n",
      "parameter to adjust between categorical and numerical attri-\n",
      "butes. Additionally, for the last three data sets (i.e., German,\n",
      "Adult, and Dermatology) which have very uneven class distribu-\n",
      "tions, the OCIL algorithm can give much improved accuracies\n",
      "compared to the other two methods. This result shows that, in\n",
      "comparison with numerically representing the distance between\n",
      "categorical values, the presented similarity metric in this paper is\n",
      "a more reasonable measurement for cluster analysis on mixed\n",
      "data and can well reveal the inherent cluster membership for\n",
      "either heterogeneous or homogeneous clusters. Moreover, com-\n",
      "paring the average running time of OCIL and k-prototypeTable 1\n",
      "Statistics of the mixed data sets.\n",
      "Data set Instance Attribute ( dcþdu)Class Class probabilities\n",
      "Statlog heart 270 7 þ6 2 55.56% 44.44%\n",
      "Heart disease 303 7 þ6 2 54.13% 45.87%\n",
      "Credit approval 653 9 þ6 2 54.67% 45.33%\n",
      "German credit 1000 13 þ7 2 70.0% 30.0%\n",
      "Adult 30,162 8 þ6 2 75.11% 24.89%\n",
      "Dermatology 366 33 þ1 6 30.6% 16.67% 19.67%\n",
      "13.39% 14.21% 5.46%\n",
      "Table 2Clustering errors of OCIL on mixed data sets in comparison with k-prototype and\n",
      "k-means.\n",
      "Data set k-Means k-Prototype OCIL\n",
      "Statlog 0.4047 70.0071 0.2306 70.0821 0.176170.0059\n",
      "Heart 0.4224 70.0131 0.2280 70.0903 0.168770.0033\n",
      "Credit 0.4487 70.0016 0.261970.0976 0.243770.0866\n",
      "German 0.3290 70.0014 0.3289 70.0006 0.3057 70.0009\n",
      "Adult 0.3869 70.0067 0.3855 70.0143 0.249070.0001\n",
      "Dermatology 0.7006 70.0216 0.690370.0255 0.302670.0973\n",
      "Table 3\n",
      "Comparison of the average convergence time between k-prototype and OCIL.\n",
      "Data set k-Prototype (s) OCIL (s)\n",
      "Statlog 0.0519 0.0498\n",
      "Heart 0.0639 0.0491\n",
      "Credit 0.1323 0.1282\n",
      "German 0.2999 0.3342\n",
      "Adult 15.2795 3.5447\n",
      "Dermatology 0.3674 0.1811\n",
      "Table 4\n",
      "Statistics of the categorical data sets.\n",
      "Data set Instance Attribute Class Class probabilities\n",
      "Soybean 47 35 4 21.28% 21.28% 21.28% 36.16%\n",
      "Breast 699 9 2 65.52% 34.48%\n",
      "Vote 435 16 2 61.38% 38.62%\n",
      "Zoo 101 16 7 40.59% 19.8% 4.95% 12.87% 3.97%\n",
      "7.92% 9.9%Y.-m. Cheung, H. Jia / Pattern Recognition 46 (2013) 2228–2238 2235\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"tell me about mixed data clustering algorithm\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_template = \"\"\"\n",
    "You are an expert at summarizing given context. The summary should at least 10 sentences and at most 25 sentences. Here is the context:\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "map_system_template = PromptTemplate.from_template(map_template)\n",
    "map_chain = map_system_template | llm\n",
    "\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "You are an expert at summarizing given context considering user prompt. \n",
    "\n",
    "Here is the user prompt:\n",
    "{user_prompt}\n",
    "\n",
    "Here is the context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "reduce_system_template = PromptTemplate.from_template(reduce_template) # ChatPromptTemplate.from_messages([(\"system\", reduce_template)])\n",
    "reduce_chain = reduce_system_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is a summary of the given context in 17 sentences:\\n\\nA clustering approach for mixed data composed of numerical and categorical attributes has been proposed, which eliminates the gap between similarity metrics for categorical and numerical data. The approach presents a general clustering framework based on object-cluster similarity and develops a unified similarity metric that can be applied to data with categorical, numerical, and mixed attributes. An iterative clustering algorithm is developed within this framework, whose performance is experimentally demonstrated on different benchmark data sets.\\n\\nThe algorithm addresses the challenging problem of selecting the number of clusters, which is crucial in clustering analysis but often not available from a practical viewpoint. The proposed approach presents a penalized competitive learning algorithm that can determine the number of clusters automatically by gradually eliminating redundant clusters.\\n\\nExperimental results show the efficacy of this improved algorithm, making it suitable for both categorical and numeric data sets. The approach overcomes the limitation of traditional clustering methods, which are not applicable to mixed data or require pre-assignment of the number of clusters.\\n\\nA general clustering framework is proposed based on object-cluster similarity, which enables a unified clustering approach that can select the cluster number automatically. The algorithm iteratively updates the centroids and assigns objects to the most similar cluster until convergence.\\n\\nThe unified similarity metric developed in this paper eliminates the gap between numerical and categorical data, allowing for a single algorithm to be applied to both types of data. This approach addresses an open problem in clustering analysis, making it more practical and effective.\\n\\nThe proposed approach is computationally efficient and suitable for large-scale data sets, making it a valuable contribution to the field of pattern recognition. The unified similarity metric developed in this paper has significant implications for various applications, including computer science and bioinformatics.',\n",
       " 'Here is a summary of the context in 23 sentences:\\n\\nThe paper proposes a new method for data clustering that can handle both numerical and categorical attributes. The method introduces a new metric for object-cluster similarity that is uniform across both types of attributes, eliminating the need for transformation and parameter adjustment between them. An iterative clustering algorithm is also introduced, which conducts a parameter-free clustering analysis and is applicable to three types of data: numerical, categorical, or mixed data. Empirical studies show that the proposed algorithm has higher accuracy and lower computational cost compared to popular k-modes algorithm for categorical data clustering. For mixed data clustering, the proposed method can get much better clustering results than k-prototype algorithm with no parameter adjustment needed.\\n\\nThe paper also presents a penalized competitive learning algorithm within the proposed clustering framework. This improved algorithm introduces competition and penalization mechanisms that can gradually fade out redundant clusters, automatically determining the number of clusters during the clustering process. Experimental results on benchmark data sets have shown the effectiveness of this method.\\n\\nA review of related works is presented in Section 2, which covers data clustering with categorical-and-numerical attributes and cluster number selection. Several methods are discussed that can be grouped into two lines: algorithms designed for purely categorical data that have been applied to mixed data via transformation, and generalized clustering criteria for numerical-and-categorical attributes.\\n\\nThe first line includes methods such as ROCK algorithm, CLICKS algorithm, and COOLCAT algorithm, which have shown superiority over traditional hierarchical algorithms but are sensitive to parameter settings. The second line includes methods such as SBAC algorithm, usm-squeezer method, and EBSC algorithm, which can handle mixed attributes but have limitations.\\n\\nThe k-prototype algorithm is also reviewed, which has been widely used for clustering categorical data but requires user-defined parameters for numerical distance and categorical distance. A simpliﬁed version of this algorithm, namely k-modes, has also been explored with improvement strategies.\\n\\nA variety of methods have been proposed for cluster number estimation, including computational demanding methods that choose the optimal number of clusters via statistic criteria, and competitive learning methods that introduce penalization and cooperation mechanisms into the clustering process. However, these existing methods focus on numerical data only and cannot be directly applied to categorical attributes.\\n\\nThe paper concludes by highlighting the importance of developing effective algorithms for data clustering with both numerical and categorical attributes, which is a nontrivial task from the practical viewpoint.',\n",
       " 'Here is a summary of the provided context in 15 sentences:\\n\\nThe clustering problem involves classifying objects into several clusters such that similarities between objects in the same group are high and similarities between objects in different groups are low. The objective function to find the optimal Qn (a partition matrix) is to maximize the similarity metric, where each object x_i belongs to a cluster C_j. The similarity metric between an object x_i and a cluster C_j can be calculated based on categorical attributes and numerical attributes separately. For mixed data, the object-cluster similarity metric s(xi,Cj) is defined as the average of the similarity calculated based on each feature. The dimensionality of x_i is d, but only dccategorical features contribute to clustering analysis. The numerical part xu_i can be treated as a whole. Each categorical attribute should be investigated individually. The weight factor wr is utilized to control the contribution of attribute-cluster similarity to object-cluster similarity. The similarity between a categorical attribute value xc_ir and Cluster C_j is defined as s(xc_ir,Cj) = sAr=xc_ir(Cj) / sAraNULL(Cj), where sAr=xc_ir(Cj) counts the number of objects in Cluster C_j that have the value xc_ir for attribute Ar. The object-cluster similarity for categorical part can be calculated by Xdc_r=1 wrs(xc_ir,Cj). This metric has properties: 0 ≤ s(xc_ir,Cj) ≤ 1, and s(xc_ir,Cj) = 1 only if all instances belonging to Cluster C_j have the value xc_ir for attribute Ar. The object-cluster similarity s(xi,Cj) can be further rewritten as dc/df * Xdc_r=1 wrs(xc_ir,Cj) + 1/df * s(xu_i,Cj), where i = 1,2,...,N and j = 1,2,...,K. The object-cluster similarity for categorical part is the weighted summation of the similarity between the cluster and each attribute value.',\n",
       " 'Here is a summary of the context in 15 sentences:\\n\\nThe object-cluster similarity metric for mixed data is defined as the weighted sum of categorical and numerical similarities. The categorical similarity is calculated using entropy metrics, which measure the information content of each attribute value. For categorical attributes with high entropy, the importance of the attribute is also high. The weight of each attribute is computed based on its entropy value. The object-cluster similarity for categorical data only is calculated by taking the weighted sum of the attribute similarities. The weights are determined by the average entropy values of the attribute values.\\n\\nThe numerical similarity metric is defined based on distance measures, such as Euclidean or Mahalanobis distances. The object-cluster similarity for purely numerical data is calculated using a similar formula to that used for categorical data. In this case, the weight is 1 and the denominator is the total number of clusters.\\n\\nThe object-cluster similarity metric is defined as the weighted sum of the categorical and numerical similarities. The weights are determined by the average entropy values of the attribute values. The object-cluster similarity metric falls within the interval [0, 1].\\n\\nFor purely categorical data, the object-cluster similarity is calculated using a formula similar to that used for mixed data. The formula involves taking the weighted sum of the attribute similarities.\\n\\nThe numerical similarity metric is equivalent to the posterior probability of an object belonging to a cluster, assuming a mixture of standard normal distributions with equal mixture coefficients. The object-cluster similarity metric can be applied to any type of data, including categorical and numerical attributes.',\n",
       " 'Here is a summary of the given context in 20 sentences:\\n\\nThe paper presents an iterative clustering algorithm based on the object-cluster similarity metric for conducting clustering analysis. The algorithm focuses on hard partition, where each object belongs to one cluster (qij = 0 or 1), although it can be extended to soft partition using posterior probability. Given a set of N objects, the optimal Qn matrix (qn_ij) can be calculated using Eq. (2). Each object xi is assigned to the cluster with the largest object-cluster similarity s(xi,Cj) among the k clusters. An iterative algorithm called OCIL (Object-Cluster Iterative Learning) is conducted to implement clustering analysis.\\n\\nAlgorithm 1: OCIL Input: data set X = {x1, x2, ..., xN}, number of clusters k Output: cluster label Y = {y1, y2, ..., yN}\\n\\nStep 1: Calculate the importance of each categorical attribute (if applicable).\\n\\nStep 2: Set Y = {0, 0, ..., 0} and select k initial objects, one for each cluster.\\n\\nRepeat until noChange is true:\\n\\nStep 3: For i = 1 to N, find y(new)_i = argmax_j [s(xi,Cj)/C1...8].\\n\\nIf y(new)_i ≠ y(old)_i, then set noChange = false and update the information of clusters C(y(new)_i) and C(y(old)_i).\\n\\nStep 4: Update the frequency of each categorical value and the centroid of numerical vectors in the new cluster.\\n\\nThe algorithm is illustrated using a three-dimensional mixed data set for clustering analysis (Fig. 1). The different point patterns represent two categorical values, and the numerical values are randomly distributed in the space [1,5] x [1,3]. Two seed points that are very close to each other were selected as the initial clusters.\\n\\nAfter one learning epoch, most data points have been assigned to a reasonable cluster except four points. During the second learning epoch, these inaccurate points are reassigned and the final result is obtained. The moving trace for the center of all numerical vectors in each cluster has also been visualized (Fig. 1). These traces give us a visual description about the change of members in the two clusters during the iterative learning process.\\n\\nThe OCIL algorithm can be used to conduct clustering analysis on mixed data sets, and its performance is influenced by the initial seed points. The random initialization method with multiple repetition was used in this paper to get the statistic information for clustering performance evaluation.',\n",
       " \"Here is a summary of the given context in 23 sentences:\\n\\nThe proposed algorithm, OCIL, has been shown to be equivalent to the k-means method when applied to purely numerical data and Euclidean distance is utilized. The clustering criterion formulated by Eq. (22) can be simplified as qnij=1 if Jxi/C0cjJ2 < rJxi/C0crJ2, otherwise 0. This means that each object will be assigned to the cluster whose centroid is closest to it.\\n\\nThe time complexity analysis of OCIL algorithm shows that the computation cost of step 1 is O(mNd c), where m is the average number of different values that can be chosen by each categorical attribute. For each iteration, the cost of the 'for' statement is O(mNkd c + Nkd u).\\n\\nThe total time cost of this algorithm is O(t(mNkd c + Nkd u)), where t is the number of iterations. From a practical viewpoint, we often have k < N, m < N, and t < N.\\n\\nSubsequently, the time complexity of this algorithm is O(dN). Therefore, the proposed algorithm is efficient for data clustering, particularly for large data sets.\\n\\nThe OCIL algorithm still suffers from a selection problem of cluster number. To overcome this problem, a penalized competitive clustering algorithm based on the object-cluster similarity metric has been presented.\\n\\nThis improved method enables automatic cluster number selection by gradually eliminating the redundant clusters. The competition mechanism is similar to that proposed in [36], where given an input xieach time, the winner Cv among the kclusters is determined by the dissimilarity between xi and each cluster as well as the winning frequency of this cluster in the past.\\n\\nThe object-cluster similarity metric s(xi,Cj) is utilized to estimate the similarity between an object and a cluster. The clustering task based on the object-cluster similarity metric can be formulated as arg max Q Xk j=1 XN i=1 qijs(xi,Cj)2, which is equivalent to finding the winning cluster.\\n\\nGiven an object xieach time, the winner Cv among the kclusters is determined by v = arg min 1rjrk[ gj(1/s(xi,Cj)) / C138], where gj is the relative winning frequency of Cjdeﬁned as nj / Pk t=1 nt.\\n\\nThe winning chance of a cluster is controlled by the object-cluster similarity as well as its winning frequency in the past competitions. Reducing the winning rate of frequent winners solves the dead-unit problem encountered by competitive learning.\\n\\nAfter selecting out the winning cluster Cv, we assign xito it and update the statistic information of Cv, which includes the center of numerical part cv and the frequency of each categorical value accompanying with xi in Cv.\\n\\nA competitive learning version of the OCIL algorithm can be summarized as Algorithm 2. The penalization mechanism enables the clustering algorithm to select the cluster number automatically during the learning process by gradually fading out the redundant clusters.\\n\\nThe proposed algorithm is efficient for data clustering, particularly for large data sets.\",\n",
       " 'Here is a summary of the given context in 23 sentences:\\n\\nThe mechanism described is used to solve the selection problem of cluster number.\\nThe basic idea of the penalization mechanism involves updating not only the winning cluster but also the rival nearest to the winner.\\nFor each input xi, the winner and its nearest rival are determined using equations (30) and (31).\\nThe weight assigned to each cluster measures the importance of each cluster to the whole cluster structure.\\nAll clusters with an equal weight have the same contribution to the cluster structure.\\nIf a cluster has a very low weight, the number of objects assigned to it will decrease and eventually this cluster will be eliminated.\\nThe winning cluster is updated based on xi, and its statistic information is also updated.\\nThe rival-penalized strength increases with the similarity between xi and the rival.\\nThe main steps of the penalized competitive learning algorithm are summarized in Algorithm 3.\\nAlgorithm 3 involves selecting k initial objects, one for each cluster, and setting Y=0,0,...,0g,nj=1 and lj=1 for j=1,2,...,k.\\nThen, the algorithm iteratively updates the winner and its nearest rival using equations (30) and (31).\\nThe weight of the winning cluster is increased, and the weight of the rival is decreased using equations (32) and (33).\\nIf the weight of a cluster decreases below a certain threshold, this cluster is regarded as redundant and neglected.\\nThe algorithm continues until no changes are made in two consecutive iterations.\\nIn the visualization of PCL-OC algorithm, the initial cluster number was set at three, and an equal weight 1.0 was assigned to each cluster.\\nThe learning rate Z was set at 0.01 to get a fast convergence speed.\\nAfter one learning epoch, the weight of the second cluster decreased because it had only one object.\\nIn the next epoch, the weight of the third cluster increased because it had two objects.\\nFinally, in the fourth epoch, the weights of all clusters were updated, and the redundant cluster was eliminated.',\n",
       " 'Here is a summary of the given context in 15 sentences:\\n\\nThe proposed object-cluster similarity metric was used to evaluate the effectiveness of clustering algorithms on various data sets from the UCI Machine Learning Data Repository. The clustering accuracy was estimated using the formula ACC = PN i=1 d(ci, map(li)) / N, where N is the number of instances in the data set and ci stands for the provided label. The clustering error rate was computed as e = 1/C0ACC. Experiments were conducted on mixed data sets to compare the performance of the proposed OCIL algorithm with k-prototype and k-means algorithms. Each algorithm was executed 100 times on each data set, and the clustering results were statistically summarized in a table. The proposed parameter-free algorithm OCIL outperformed the k-prototype and k-means methods in terms of clustering accuracy. The ratios of categorical attributes to numerical attributes in the utilized data sets were different from each other, but the OCIL algorithm achieved satisfactory clustering results. For the last three data sets (German, Adult, and Dermatology) which have very uneven class distributions, the OCIL algorithm gave much improved accuracies compared to the other two methods. The presented similarity metric is a more reasonable measurement for cluster analysis on mixed data and can well reveal the inherent cluster membership for either heterogeneous or homogeneous clusters. The average running time of OCIL was compared with k-prototype, and it showed that OCIL converges faster than k-prototype for most data sets. The statistics of the categorical data sets were provided in a table, showing the number of instances, attributes, and class probabilities for each data set. The performance of the proposed algorithm on categorical data sets will be evaluated in future research. The goal is to develop an efficient and accurate clustering algorithm that can handle complex and high-dimensional data. The results show that the proposed OCIL algorithm outperforms existing algorithms in terms of accuracy and speed, making it a promising solution for clustering tasks.',\n",
       " \"Here is the summary in 20 sentences:\\n\\nThe performance of OCIL algorithm was compared to k-prototype and other two existing categorical data clustering algorithms: original k-modes (H's k-modes) and k-modes with Ng's dissimilarity metric (N's k-modes). The results showed that OCIL had a competitive advantage in terms of clustering accuracy and robustness. The proposed clustering method, OCIL, was found to be superior to the other two methods due to its object-cluster similarity metric. This metric considers the distribution of attribute values within a cluster when calculating similarity. In contrast, N's k-modes uses frequencies of attribute values if the object and cluster mode have the same values. However, when the object and cluster mode have different attribute values, N's k-modes assumes a distance of 1. OCIL also does not use a cluster mode to represent each cluster but calculates similarity based on the cluster's statistic information. This allows for more accurate representation of clusters in categorical data. The convergence speed of OCIL was faster than k-modes with an average improvement of 60% in all cases tried. The computational cost of OCIL was also less than that of k-modes because OCIL does not update cluster modes in each learning step. To further evaluate the effectiveness of PCL-OC algorithm, experiments were conducted on real data sets including purely categorical and mixed data. The results showed that PCL-OC had outstanding performance compared to k-prototype. PCL-OC can automatically determine the number of clusters without prior knowledge. The learning rate in the penalization mechanism was set as r(k/N), where r is a small coefficient and N is the number of objects in the given data set. A rule of thumb for setting the value of r was provided based on the size of the data set. For Soybean data with 47 instances, r can be set between 0.001 and 0.003. For other data sets with hundreds of instances, r can be set between 0.003 and 0.006. Experiments were conducted to show PCL-OC's ability of automatical cluster number selection on real data set. The results showed that PCL-OC successfully selected the optimal number of clusters.\",\n",
       " \"It seems like you've provided a reference list for various papers on clustering categorical data and a table comparing the results of PCL-OC with k-prototype algorithm on different datasets.\\n\\nHere's a summary:\\n\\n**Reference List:**\\n\\nThe reference list includes papers related to clustering categorical data, such as COOLCAT [16], LIMBO [17], information bottleneck method [18], and others. These papers propose algorithms for clustering large datasets with mixed type attributes, including categorical values.\\n\\n**Table 7:**\\n\\nThe table compares the results of PCL-OC (a proposed algorithm) with k-prototype algorithm on different datasets, including Heart, Credit, Soybean, and Vote. The table presents the average error rate and standard deviation (std) for each dataset and algorithm combination.\\n\\nThe results show that PCL-OC generally performs better than k-prototype algorithm in terms of error rate and standard deviation across all datasets.\\n\\nPlease let me know if you'd like me to elaborate on any specific aspect or clarify any details!\",\n",
       " \"Here is a summary of the given context in 20 sentences:\\n\\nThe papers [27-37] are related to data clustering, machine learning, and pattern recognition. The first paper, [27], discusses the impact of dissimilarity measures on the k-modes clustering algorithm. In [28], Akaike presents a new approach to statistical model identification. Schwarz's work in [29] focuses on estimating the dimension of a model.\\n\\nCheung's research in [30] involves a competitive and cooperative learning approach to robust data clustering. Filippone et al.'s survey in [31] covers kernel and spectral methods for clustering. Jain's paper in [32] provides an overview of 50 years of progress in data clustering beyond k-means.\\n\\nBasak and Krishnapuram's work in [33] proposes an interpretable hierarchical clustering method using an unsupervised decision tree. In [34], Lozano et al. compare four initialization methods for the k-means algorithm. Khan and Ahmad's paper in [35] presents a cluster center initialization algorithm for k-means clustering.\\n\\nAhalt et al.'s research in [36] involves competitive learning algorithms for vector quantization. He et al.'s work in [37] proposes a Laplacian score method for feature selection. Yiu-ming Cheung is a professor at the Department of Computer Science, Hong Kong Baptist University, and has research interests in machine learning and information security.\\n\\nCheung received his Ph.D. from The Chinese University of Hong Kong in 2000 and joined Hong Kong Baptist University in 2001. He is also the founding chairman of IEEE (Hong Kong) Computational Intelligence Chapter and associate editor of Knowledge and Information Systems.\\n\\nHong Jia is a P.hD student at the Department of Computer Science, Hong Kong Baptist University, and her research interests include clustering analysis and pattern recognition.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_summaries = []\n",
    "\n",
    "for doc in docs:\n",
    "    response = map_chain.invoke({\"context\": doc.page_content})\n",
    "    page_summaries.append(response.content)\n",
    "    \n",
    "page_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the given context in 17 sentences:\n",
      "\n",
      "A clustering approach for mixed data composed of numerical and categorical attributes has been proposed, which eliminates the gap between similarity metrics for categorical and numerical data. The approach presents a general clustering framework based on object-cluster similarity and develops a unified similarity metric that can be applied to data with categorical, numerical, and mixed attributes. An iterative clustering algorithm is developed within this framework, whose performance is experimentally demonstrated on different benchmark data sets.\n",
      "\n",
      "The algorithm addresses the challenging problem of selecting the number of clusters, which is crucial in clustering analysis but often not available from a practical viewpoint. The proposed approach presents a penalized competitive learning algorithm that can determine the number of clusters automatically by gradually eliminating redundant clusters.\n",
      "\n",
      "Experimental results show the efficacy of this improved algorithm, making it suitable for both categorical and numeric data sets. The approach overcomes the limitation of traditional clustering methods, which are not applicable to mixed data or require pre-assignment of the number of clusters.\n",
      "\n",
      "A general clustering framework is proposed based on object-cluster similarity, which enables a unified clustering approach that can select the cluster number automatically. The algorithm iteratively updates the centroids and assigns objects to the most similar cluster until convergence.\n",
      "\n",
      "The unified similarity metric developed in this paper eliminates the gap between numerical and categorical data, allowing for a single algorithm to be applied to both types of data. This approach addresses an open problem in clustering analysis, making it more practical and effective.\n",
      "\n",
      "The proposed approach is computationally efficient and suitable for large-scale data sets, making it a valuable contribution to the field of pattern recognition. The unified similarity metric developed in this paper has significant implications for various applications, including computer science and bioinformatics./n Here is a summary of the context in 23 sentences:\n",
      "\n",
      "The paper proposes a new method for data clustering that can handle both numerical and categorical attributes. The method introduces a new metric for object-cluster similarity that is uniform across both types of attributes, eliminating the need for transformation and parameter adjustment between them. An iterative clustering algorithm is also introduced, which conducts a parameter-free clustering analysis and is applicable to three types of data: numerical, categorical, or mixed data. Empirical studies show that the proposed algorithm has higher accuracy and lower computational cost compared to popular k-modes algorithm for categorical data clustering. For mixed data clustering, the proposed method can get much better clustering results than k-prototype algorithm with no parameter adjustment needed.\n",
      "\n",
      "The paper also presents a penalized competitive learning algorithm within the proposed clustering framework. This improved algorithm introduces competition and penalization mechanisms that can gradually fade out redundant clusters, automatically determining the number of clusters during the clustering process. Experimental results on benchmark data sets have shown the effectiveness of this method.\n",
      "\n",
      "A review of related works is presented in Section 2, which covers data clustering with categorical-and-numerical attributes and cluster number selection. Several methods are discussed that can be grouped into two lines: algorithms designed for purely categorical data that have been applied to mixed data via transformation, and generalized clustering criteria for numerical-and-categorical attributes.\n",
      "\n",
      "The first line includes methods such as ROCK algorithm, CLICKS algorithm, and COOLCAT algorithm, which have shown superiority over traditional hierarchical algorithms but are sensitive to parameter settings. The second line includes methods such as SBAC algorithm, usm-squeezer method, and EBSC algorithm, which can handle mixed attributes but have limitations.\n",
      "\n",
      "The k-prototype algorithm is also reviewed, which has been widely used for clustering categorical data but requires user-defined parameters for numerical distance and categorical distance. A simpliﬁed version of this algorithm, namely k-modes, has also been explored with improvement strategies.\n",
      "\n",
      "A variety of methods have been proposed for cluster number estimation, including computational demanding methods that choose the optimal number of clusters via statistic criteria, and competitive learning methods that introduce penalization and cooperation mechanisms into the clustering process. However, these existing methods focus on numerical data only and cannot be directly applied to categorical attributes.\n",
      "\n",
      "The paper concludes by highlighting the importance of developing effective algorithms for data clustering with both numerical and categorical attributes, which is a nontrivial task from the practical viewpoint./n Here is a summary of the provided context in 15 sentences:\n",
      "\n",
      "The clustering problem involves classifying objects into several clusters such that similarities between objects in the same group are high and similarities between objects in different groups are low. The objective function to find the optimal Qn (a partition matrix) is to maximize the similarity metric, where each object x_i belongs to a cluster C_j. The similarity metric between an object x_i and a cluster C_j can be calculated based on categorical attributes and numerical attributes separately. For mixed data, the object-cluster similarity metric s(xi,Cj) is defined as the average of the similarity calculated based on each feature. The dimensionality of x_i is d, but only dccategorical features contribute to clustering analysis. The numerical part xu_i can be treated as a whole. Each categorical attribute should be investigated individually. The weight factor wr is utilized to control the contribution of attribute-cluster similarity to object-cluster similarity. The similarity between a categorical attribute value xc_ir and Cluster C_j is defined as s(xc_ir,Cj) = sAr=xc_ir(Cj) / sAraNULL(Cj), where sAr=xc_ir(Cj) counts the number of objects in Cluster C_j that have the value xc_ir for attribute Ar. The object-cluster similarity for categorical part can be calculated by Xdc_r=1 wrs(xc_ir,Cj). This metric has properties: 0 ≤ s(xc_ir,Cj) ≤ 1, and s(xc_ir,Cj) = 1 only if all instances belonging to Cluster C_j have the value xc_ir for attribute Ar. The object-cluster similarity s(xi,Cj) can be further rewritten as dc/df * Xdc_r=1 wrs(xc_ir,Cj) + 1/df * s(xu_i,Cj), where i = 1,2,...,N and j = 1,2,...,K. The object-cluster similarity for categorical part is the weighted summation of the similarity between the cluster and each attribute value./n Here is a summary of the context in 15 sentences:\n",
      "\n",
      "The object-cluster similarity metric for mixed data is defined as the weighted sum of categorical and numerical similarities. The categorical similarity is calculated using entropy metrics, which measure the information content of each attribute value. For categorical attributes with high entropy, the importance of the attribute is also high. The weight of each attribute is computed based on its entropy value. The object-cluster similarity for categorical data only is calculated by taking the weighted sum of the attribute similarities. The weights are determined by the average entropy values of the attribute values.\n",
      "\n",
      "The numerical similarity metric is defined based on distance measures, such as Euclidean or Mahalanobis distances. The object-cluster similarity for purely numerical data is calculated using a similar formula to that used for categorical data. In this case, the weight is 1 and the denominator is the total number of clusters.\n",
      "\n",
      "The object-cluster similarity metric is defined as the weighted sum of the categorical and numerical similarities. The weights are determined by the average entropy values of the attribute values. The object-cluster similarity metric falls within the interval [0, 1].\n",
      "\n",
      "For purely categorical data, the object-cluster similarity is calculated using a formula similar to that used for mixed data. The formula involves taking the weighted sum of the attribute similarities.\n",
      "\n",
      "The numerical similarity metric is equivalent to the posterior probability of an object belonging to a cluster, assuming a mixture of standard normal distributions with equal mixture coefficients. The object-cluster similarity metric can be applied to any type of data, including categorical and numerical attributes./n Here is a summary of the given context in 20 sentences:\n",
      "\n",
      "The paper presents an iterative clustering algorithm based on the object-cluster similarity metric for conducting clustering analysis. The algorithm focuses on hard partition, where each object belongs to one cluster (qij = 0 or 1), although it can be extended to soft partition using posterior probability. Given a set of N objects, the optimal Qn matrix (qn_ij) can be calculated using Eq. (2). Each object xi is assigned to the cluster with the largest object-cluster similarity s(xi,Cj) among the k clusters. An iterative algorithm called OCIL (Object-Cluster Iterative Learning) is conducted to implement clustering analysis.\n",
      "\n",
      "Algorithm 1: OCIL Input: data set X = {x1, x2, ..., xN}, number of clusters k Output: cluster label Y = {y1, y2, ..., yN}\n",
      "\n",
      "Step 1: Calculate the importance of each categorical attribute (if applicable).\n",
      "\n",
      "Step 2: Set Y = {0, 0, ..., 0} and select k initial objects, one for each cluster.\n",
      "\n",
      "Repeat until noChange is true:\n",
      "\n",
      "Step 3: For i = 1 to N, find y(new)_i = argmax_j [s(xi,Cj)/C1...8].\n",
      "\n",
      "If y(new)_i ≠ y(old)_i, then set noChange = false and update the information of clusters C(y(new)_i) and C(y(old)_i).\n",
      "\n",
      "Step 4: Update the frequency of each categorical value and the centroid of numerical vectors in the new cluster.\n",
      "\n",
      "The algorithm is illustrated using a three-dimensional mixed data set for clustering analysis (Fig. 1). The different point patterns represent two categorical values, and the numerical values are randomly distributed in the space [1,5] x [1,3]. Two seed points that are very close to each other were selected as the initial clusters.\n",
      "\n",
      "After one learning epoch, most data points have been assigned to a reasonable cluster except four points. During the second learning epoch, these inaccurate points are reassigned and the final result is obtained. The moving trace for the center of all numerical vectors in each cluster has also been visualized (Fig. 1). These traces give us a visual description about the change of members in the two clusters during the iterative learning process.\n",
      "\n",
      "The OCIL algorithm can be used to conduct clustering analysis on mixed data sets, and its performance is influenced by the initial seed points. The random initialization method with multiple repetition was used in this paper to get the statistic information for clustering performance evaluation./n Here is a summary of the given context in 23 sentences:\n",
      "\n",
      "The proposed algorithm, OCIL, has been shown to be equivalent to the k-means method when applied to purely numerical data and Euclidean distance is utilized. The clustering criterion formulated by Eq. (22) can be simplified as qnij=1 if Jxi/C0cjJ2 < rJxi/C0crJ2, otherwise 0. This means that each object will be assigned to the cluster whose centroid is closest to it.\n",
      "\n",
      "The time complexity analysis of OCIL algorithm shows that the computation cost of step 1 is O(mNd c), where m is the average number of different values that can be chosen by each categorical attribute. For each iteration, the cost of the 'for' statement is O(mNkd c + Nkd u).\n",
      "\n",
      "The total time cost of this algorithm is O(t(mNkd c + Nkd u)), where t is the number of iterations. From a practical viewpoint, we often have k < N, m < N, and t < N.\n",
      "\n",
      "Subsequently, the time complexity of this algorithm is O(dN). Therefore, the proposed algorithm is efficient for data clustering, particularly for large data sets.\n",
      "\n",
      "The OCIL algorithm still suffers from a selection problem of cluster number. To overcome this problem, a penalized competitive clustering algorithm based on the object-cluster similarity metric has been presented.\n",
      "\n",
      "This improved method enables automatic cluster number selection by gradually eliminating the redundant clusters. The competition mechanism is similar to that proposed in [36], where given an input xieach time, the winner Cv among the kclusters is determined by the dissimilarity between xi and each cluster as well as the winning frequency of this cluster in the past.\n",
      "\n",
      "The object-cluster similarity metric s(xi,Cj) is utilized to estimate the similarity between an object and a cluster. The clustering task based on the object-cluster similarity metric can be formulated as arg max Q Xk j=1 XN i=1 qijs(xi,Cj)2, which is equivalent to finding the winning cluster.\n",
      "\n",
      "Given an object xieach time, the winner Cv among the kclusters is determined by v = arg min 1rjrk[ gj(1/s(xi,Cj)) / C138], where gj is the relative winning frequency of Cjdeﬁned as nj / Pk t=1 nt.\n",
      "\n",
      "The winning chance of a cluster is controlled by the object-cluster similarity as well as its winning frequency in the past competitions. Reducing the winning rate of frequent winners solves the dead-unit problem encountered by competitive learning.\n",
      "\n",
      "After selecting out the winning cluster Cv, we assign xito it and update the statistic information of Cv, which includes the center of numerical part cv and the frequency of each categorical value accompanying with xi in Cv.\n",
      "\n",
      "A competitive learning version of the OCIL algorithm can be summarized as Algorithm 2. The penalization mechanism enables the clustering algorithm to select the cluster number automatically during the learning process by gradually fading out the redundant clusters.\n",
      "\n",
      "The proposed algorithm is efficient for data clustering, particularly for large data sets./n Here is a summary of the given context in 23 sentences:\n",
      "\n",
      "The mechanism described is used to solve the selection problem of cluster number.\n",
      "The basic idea of the penalization mechanism involves updating not only the winning cluster but also the rival nearest to the winner.\n",
      "For each input xi, the winner and its nearest rival are determined using equations (30) and (31).\n",
      "The weight assigned to each cluster measures the importance of each cluster to the whole cluster structure.\n",
      "All clusters with an equal weight have the same contribution to the cluster structure.\n",
      "If a cluster has a very low weight, the number of objects assigned to it will decrease and eventually this cluster will be eliminated.\n",
      "The winning cluster is updated based on xi, and its statistic information is also updated.\n",
      "The rival-penalized strength increases with the similarity between xi and the rival.\n",
      "The main steps of the penalized competitive learning algorithm are summarized in Algorithm 3.\n",
      "Algorithm 3 involves selecting k initial objects, one for each cluster, and setting Y=0,0,...,0g,nj=1 and lj=1 for j=1,2,...,k.\n",
      "Then, the algorithm iteratively updates the winner and its nearest rival using equations (30) and (31).\n",
      "The weight of the winning cluster is increased, and the weight of the rival is decreased using equations (32) and (33).\n",
      "If the weight of a cluster decreases below a certain threshold, this cluster is regarded as redundant and neglected.\n",
      "The algorithm continues until no changes are made in two consecutive iterations.\n",
      "In the visualization of PCL-OC algorithm, the initial cluster number was set at three, and an equal weight 1.0 was assigned to each cluster.\n",
      "The learning rate Z was set at 0.01 to get a fast convergence speed.\n",
      "After one learning epoch, the weight of the second cluster decreased because it had only one object.\n",
      "In the next epoch, the weight of the third cluster increased because it had two objects.\n",
      "Finally, in the fourth epoch, the weights of all clusters were updated, and the redundant cluster was eliminated./n Here is a summary of the given context in 15 sentences:\n",
      "\n",
      "The proposed object-cluster similarity metric was used to evaluate the effectiveness of clustering algorithms on various data sets from the UCI Machine Learning Data Repository. The clustering accuracy was estimated using the formula ACC = PN i=1 d(ci, map(li)) / N, where N is the number of instances in the data set and ci stands for the provided label. The clustering error rate was computed as e = 1/C0ACC. Experiments were conducted on mixed data sets to compare the performance of the proposed OCIL algorithm with k-prototype and k-means algorithms. Each algorithm was executed 100 times on each data set, and the clustering results were statistically summarized in a table. The proposed parameter-free algorithm OCIL outperformed the k-prototype and k-means methods in terms of clustering accuracy. The ratios of categorical attributes to numerical attributes in the utilized data sets were different from each other, but the OCIL algorithm achieved satisfactory clustering results. For the last three data sets (German, Adult, and Dermatology) which have very uneven class distributions, the OCIL algorithm gave much improved accuracies compared to the other two methods. The presented similarity metric is a more reasonable measurement for cluster analysis on mixed data and can well reveal the inherent cluster membership for either heterogeneous or homogeneous clusters. The average running time of OCIL was compared with k-prototype, and it showed that OCIL converges faster than k-prototype for most data sets. The statistics of the categorical data sets were provided in a table, showing the number of instances, attributes, and class probabilities for each data set. The performance of the proposed algorithm on categorical data sets will be evaluated in future research. The goal is to develop an efficient and accurate clustering algorithm that can handle complex and high-dimensional data. The results show that the proposed OCIL algorithm outperforms existing algorithms in terms of accuracy and speed, making it a promising solution for clustering tasks./n Here is the summary in 20 sentences:\n",
      "\n",
      "The performance of OCIL algorithm was compared to k-prototype and other two existing categorical data clustering algorithms: original k-modes (H's k-modes) and k-modes with Ng's dissimilarity metric (N's k-modes). The results showed that OCIL had a competitive advantage in terms of clustering accuracy and robustness. The proposed clustering method, OCIL, was found to be superior to the other two methods due to its object-cluster similarity metric. This metric considers the distribution of attribute values within a cluster when calculating similarity. In contrast, N's k-modes uses frequencies of attribute values if the object and cluster mode have the same values. However, when the object and cluster mode have different attribute values, N's k-modes assumes a distance of 1. OCIL also does not use a cluster mode to represent each cluster but calculates similarity based on the cluster's statistic information. This allows for more accurate representation of clusters in categorical data. The convergence speed of OCIL was faster than k-modes with an average improvement of 60% in all cases tried. The computational cost of OCIL was also less than that of k-modes because OCIL does not update cluster modes in each learning step. To further evaluate the effectiveness of PCL-OC algorithm, experiments were conducted on real data sets including purely categorical and mixed data. The results showed that PCL-OC had outstanding performance compared to k-prototype. PCL-OC can automatically determine the number of clusters without prior knowledge. The learning rate in the penalization mechanism was set as r(k/N), where r is a small coefficient and N is the number of objects in the given data set. A rule of thumb for setting the value of r was provided based on the size of the data set. For Soybean data with 47 instances, r can be set between 0.001 and 0.003. For other data sets with hundreds of instances, r can be set between 0.003 and 0.006. Experiments were conducted to show PCL-OC's ability of automatical cluster number selection on real data set. The results showed that PCL-OC successfully selected the optimal number of clusters./n It seems like you've provided a reference list for various papers on clustering categorical data and a table comparing the results of PCL-OC with k-prototype algorithm on different datasets.\n",
      "\n",
      "Here's a summary:\n",
      "\n",
      "**Reference List:**\n",
      "\n",
      "The reference list includes papers related to clustering categorical data, such as COOLCAT [16], LIMBO [17], information bottleneck method [18], and others. These papers propose algorithms for clustering large datasets with mixed type attributes, including categorical values.\n",
      "\n",
      "**Table 7:**\n",
      "\n",
      "The table compares the results of PCL-OC (a proposed algorithm) with k-prototype algorithm on different datasets, including Heart, Credit, Soybean, and Vote. The table presents the average error rate and standard deviation (std) for each dataset and algorithm combination.\n",
      "\n",
      "The results show that PCL-OC generally performs better than k-prototype algorithm in terms of error rate and standard deviation across all datasets.\n",
      "\n",
      "Please let me know if you'd like me to elaborate on any specific aspect or clarify any details!/n Here is a summary of the given context in 20 sentences:\n",
      "\n",
      "The papers [27-37] are related to data clustering, machine learning, and pattern recognition. The first paper, [27], discusses the impact of dissimilarity measures on the k-modes clustering algorithm. In [28], Akaike presents a new approach to statistical model identification. Schwarz's work in [29] focuses on estimating the dimension of a model.\n",
      "\n",
      "Cheung's research in [30] involves a competitive and cooperative learning approach to robust data clustering. Filippone et al.'s survey in [31] covers kernel and spectral methods for clustering. Jain's paper in [32] provides an overview of 50 years of progress in data clustering beyond k-means.\n",
      "\n",
      "Basak and Krishnapuram's work in [33] proposes an interpretable hierarchical clustering method using an unsupervised decision tree. In [34], Lozano et al. compare four initialization methods for the k-means algorithm. Khan and Ahmad's paper in [35] presents a cluster center initialization algorithm for k-means clustering.\n",
      "\n",
      "Ahalt et al.'s research in [36] involves competitive learning algorithms for vector quantization. He et al.'s work in [37] proposes a Laplacian score method for feature selection. Yiu-ming Cheung is a professor at the Department of Computer Science, Hong Kong Baptist University, and has research interests in machine learning and information security.\n",
      "\n",
      "Cheung received his Ph.D. from The Chinese University of Hong Kong in 2000 and joined Hong Kong Baptist University in 2001. He is also the founding chairman of IEEE (Hong Kong) Computational Intelligence Chapter and associate editor of Knowledge and Information Systems.\n",
      "\n",
      "Hong Jia is a P.hD student at the Department of Computer Science, Hong Kong Baptist University, and her research interests include clustering analysis and pattern recognition.\n"
     ]
    }
   ],
   "source": [
    "page_summaries_str = \"/n \".join(page_summaries)\n",
    "print(page_summaries_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you've provided a summary of various papers related to data clustering and machine learning. Here's a breakdown of the 20 sentences:\n",
      "\n",
      "**Papers and Research**\n",
      "\n",
      "* The first paper [27] discusses the impact of dissimilarity measures on the k-modes clustering algorithm.\n",
      "* Paper [28] by Akaike presents a new approach to statistical model identification.\n",
      "* Schwarz's work in [29] focuses on estimating the dimension of a model.\n",
      "* Cheung's research in [30] involves a competitive and cooperative learning approach to robust data clustering.\n",
      "\n",
      "**Survey Papers**\n",
      "\n",
      "* Filippone et al.'s survey in [31] covers kernel and spectral methods for clustering.\n",
      "* Jain's paper in [32] provides an overview of 50 years of progress in data clustering beyond k-means.\n",
      "\n",
      "**New Approaches**\n",
      "\n",
      "* Basak and Krishnapuram's work in [33] proposes an interpretable hierarchical clustering method using an unsupervised decision tree.\n",
      "* Lozano et al.'s research in [34] compares four initialization methods for the k-means algorithm.\n",
      "* Khan and Ahmad's paper in [35] presents a cluster center initialization algorithm for k-means clustering.\n",
      "\n",
      "**Competitive Learning**\n",
      "\n",
      "* Ahalt et al.'s research in [36] involves competitive learning algorithms for vector quantization.\n",
      "\n",
      "**Feature Selection**\n",
      "\n",
      "* He et al.'s work in [37] proposes a Laplacian score method for feature selection.\n",
      "\n",
      "**Researchers and Institutions**\n",
      "\n",
      "* Yiu-ming Cheung is a professor at the Department of Computer Science, Hong Kong Baptist University.\n",
      "* Cheung received his Ph.D. from The Chinese University of Hong Kong in 2000.\n",
      "* Hong Jia is a P.hD student at the Department of Computer Science, Hong Kong Baptist University.\n",
      "\n",
      "Let me know if you'd like me to clarify any specific point or provide further details!\n"
     ]
    }
   ],
   "source": [
    "final_response = reduce_chain.invoke({\"user_prompt\": \"Give me a comprehensive summmary with all key points.\",\n",
    "                                      \"context\": page_summaries_str})\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool/Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_web(query: str, max_results: Optional[int] = 5) -> List:\n",
    "    \"\"\"\n",
    "    Search for text on duckduckgo.com.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search for.\n",
    "        max_results Optional[int]: The maximum number of search results to retrieve (default 5).\n",
    "    Returns:\n",
    "        List of search results as strings.\n",
    "    \"\"\"    \n",
    "    # Searching for pdf files\n",
    "    with DDGS() as ddgs:\n",
    "        results = [r for r in ddgs.text(f'{query}', region='wt-wt', safesearch='off', max_results=max_results)]\n",
    "    return results\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools=[search_web])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpfull assistant. Here is the user prompt: {user_prompt}\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt_template | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"user_prompt\": \"what is the latest openai model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', response_metadata={'model': 'llama3-groq-tool-use:8b', 'created_at': '2024-07-26T09:34:33.659459Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'search_web', 'arguments': {'query': 'latest OpenAI model'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 670534500, 'load_duration': 26419250, 'prompt_eval_count': 221, 'prompt_eval_duration': 218982000, 'eval_count': 26, 'eval_duration': 424005000}, id='run-db2565e2-832e-4c57-b5ad-98134c3d45d9-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'latest OpenAI model'}, 'id': 'cd6ac7a1-b7c7-4001-bb04-393d75ce83da', 'type': 'tool_call'}], usage_metadata={'input_tokens': 221, 'output_tokens': 26, 'total_tokens': 247})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'search_web',\n",
       "  'args': {'query': 'latest OpenAI model'},\n",
       "  'id': 'cd6ac7a1-b7c7-4001-bb04-393d75ce83da',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'latest OpenAI model'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_response(response):\n",
    "    try:\n",
    "        if response.tool_calls!=[]:\n",
    "            tool_args = response.tool_calls[0][\"args\"]\n",
    "            search_results = search_web.invoke(tool_args)\n",
    "            return search_results\n",
    "        else:\n",
    "            return response.content\n",
    "    except:\n",
    "        return response.content\n",
    "\n",
    "\n",
    "template = \"You are a helpfull assistant. Here is the user prompt: {user_prompt}\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "chain_with_runtool = prompt_template | llm_with_tools | RunnableLambda(check_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = chain_with_runtool.invoke({\"user_prompt\": \"get latest news about latest openai model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'OpenAI unveils newest AI model, GPT-4o | CNN Business',\n",
       "  'href': 'https://www.cnn.com/2024/05/13/tech/openai-altman-new-ai-model-gpt-4o/index.html',\n",
       "  'body': 'OpenAI on Monday announced its latest artificial intelligence large language model that it says will make ChatGPT smarter and easier to use. The new model, called GPT-4o, is an update from the ...'},\n",
       " {'title': 'OpenAI Says It Has Begun Training a New Flagship A.I. Model',\n",
       "  'href': 'https://www.nytimes.com/2024/05/28/technology/openai-gpt4-new-model.html',\n",
       "  'body': 'May 28, 2024. OpenAI said on Tuesday that it had begun training a new flagship artificial intelligence model that would succeed the GPT-4 technology that drives its popular online chatbot, ChatGPT ...'},\n",
       " {'title': 'Exclusive: OpenAI working on new reasoning technology under code name ...',\n",
       "  'href': 'https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/',\n",
       "  'body': 'July 15, 20247:37 AM PDTUpdated 8 days ago. July 12 - ChatGPT maker OpenAI is working on a novel approach to its artificial intelligence models in a project code-named \"Strawberry,\" according ...'},\n",
       " {'title': \"OpenAI Starts Training Next AI Model — Here's What We Know - Forbes\",\n",
       "  'href': 'https://www.forbes.com/sites/roberthart/2024/05/28/openai-says-it-has-started-training-gpt-4-successor---heres-what-we-know/',\n",
       "  'body': 'Topline. OpenAI has created a new safety and security board and has started work training its next AI model, the company announced Tuesday in a blog post, a rare insight into the hotly anticipated ...'},\n",
       " {'title': 'New embedding models and API updates | OpenAI',\n",
       "  'href': 'https://openai.com/blog/new-embedding-models-and-api-updates/',\n",
       "  'body': 'We are launching a new generation of embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT-3.5 Turbo. We are releasing new models, reducing prices for GPT-3.5 Turbo, and introducing new ways for developers to manage API keys and understand API usage. The new models include:'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
